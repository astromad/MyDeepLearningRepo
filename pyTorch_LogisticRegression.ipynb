{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyTorch_LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOEnCuELkRbt5OyOFhjhai0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astromad/MyDeepLearningRepo/blob/master/pyTorch_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXiOtR3l8_DY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5214237-6b01-44d0-8499-8f22f1db7b40"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0],[4.0]])\n",
        "y_data = tensor([[0.0], [0.0], [1.0],[1.0]])\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        y_pred = F.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.BCELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(500):\n",
        "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 2) Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# After training\n",
        "hour_var = tensor([[1.0]])\n",
        "y_pred = model(hour_var)\n",
        "print('prediction',y_pred.item(),model(hour_var).data.item())\n",
        "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item()> 0.5)\n",
        "hour_var = tensor([[7.0]])\n",
        "y_pred = model(hour_var)\n",
        "print('prediction',y_pred.item(),model(hour_var).data[0][0].item())\n",
        "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item()> 0.5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 2.1611125469207764 \n",
            "Epoch: 1 | Loss: 2.1451287269592285 \n",
            "Epoch: 2 | Loss: 2.131134510040283 \n",
            "Epoch: 3 | Loss: 2.1188149452209473 \n",
            "Epoch: 4 | Loss: 2.107908010482788 \n",
            "Epoch: 5 | Loss: 2.098195791244507 \n",
            "Epoch: 6 | Loss: 2.089494228363037 \n",
            "Epoch: 7 | Loss: 2.081650972366333 \n",
            "Epoch: 8 | Loss: 2.0745370388031006 \n",
            "Epoch: 9 | Loss: 2.0680437088012695 \n",
            "Epoch: 10 | Loss: 2.0620803833007812 \n",
            "Epoch: 11 | Loss: 2.0565695762634277 \n",
            "Epoch: 12 | Loss: 2.0514464378356934 \n",
            "Epoch: 13 | Loss: 2.0466561317443848 \n",
            "Epoch: 14 | Loss: 2.04215145111084 \n",
            "Epoch: 15 | Loss: 2.0378928184509277 \n",
            "Epoch: 16 | Loss: 2.033845901489258 \n",
            "Epoch: 17 | Loss: 2.029982566833496 \n",
            "Epoch: 18 | Loss: 2.026277542114258 \n",
            "Epoch: 19 | Loss: 2.0227103233337402 \n",
            "Epoch: 20 | Loss: 2.0192625522613525 \n",
            "Epoch: 21 | Loss: 2.0159189701080322 \n",
            "Epoch: 22 | Loss: 2.0126657485961914 \n",
            "Epoch: 23 | Loss: 2.0094923973083496 \n",
            "Epoch: 24 | Loss: 2.0063886642456055 \n",
            "Epoch: 25 | Loss: 2.0033457279205322 \n",
            "Epoch: 26 | Loss: 2.000356674194336 \n",
            "Epoch: 27 | Loss: 1.9974156618118286 \n",
            "Epoch: 28 | Loss: 1.9945166110992432 \n",
            "Epoch: 29 | Loss: 1.991654872894287 \n",
            "Epoch: 30 | Loss: 1.988827109336853 \n",
            "Epoch: 31 | Loss: 1.986029028892517 \n",
            "Epoch: 32 | Loss: 1.9832581281661987 \n",
            "Epoch: 33 | Loss: 1.98051118850708 \n",
            "Epoch: 34 | Loss: 1.9777865409851074 \n",
            "Epoch: 35 | Loss: 1.9750820398330688 \n",
            "Epoch: 36 | Loss: 1.972395658493042 \n",
            "Epoch: 37 | Loss: 1.9697262048721313 \n",
            "Epoch: 38 | Loss: 1.9670722484588623 \n",
            "Epoch: 39 | Loss: 1.964432954788208 \n",
            "Epoch: 40 | Loss: 1.9618068933486938 \n",
            "Epoch: 41 | Loss: 1.959193468093872 \n",
            "Epoch: 42 | Loss: 1.9565918445587158 \n",
            "Epoch: 43 | Loss: 1.954001545906067 \n",
            "Epoch: 44 | Loss: 1.9514217376708984 \n",
            "Epoch: 45 | Loss: 1.9488518238067627 \n",
            "Epoch: 46 | Loss: 1.9462918043136597 \n",
            "Epoch: 47 | Loss: 1.9437410831451416 \n",
            "Epoch: 48 | Loss: 1.9411990642547607 \n",
            "Epoch: 49 | Loss: 1.9386658668518066 \n",
            "Epoch: 50 | Loss: 1.936141014099121 \n",
            "Epoch: 51 | Loss: 1.933624267578125 \n",
            "Epoch: 52 | Loss: 1.9311153888702393 \n",
            "Epoch: 53 | Loss: 1.9286142587661743 \n",
            "Epoch: 54 | Loss: 1.9261207580566406 \n",
            "Epoch: 55 | Loss: 1.9236347675323486 \n",
            "Epoch: 56 | Loss: 1.9211559295654297 \n",
            "Epoch: 57 | Loss: 1.918684482574463 \n",
            "Epoch: 58 | Loss: 1.9162198305130005 \n",
            "Epoch: 59 | Loss: 1.9137628078460693 \n",
            "Epoch: 60 | Loss: 1.9113121032714844 \n",
            "Epoch: 61 | Loss: 1.9088685512542725 \n",
            "Epoch: 62 | Loss: 1.9064316749572754 \n",
            "Epoch: 63 | Loss: 1.9040017127990723 \n",
            "Epoch: 64 | Loss: 1.9015781879425049 \n",
            "Epoch: 65 | Loss: 1.8991612195968628 \n",
            "Epoch: 66 | Loss: 1.8967511653900146 \n",
            "Epoch: 67 | Loss: 1.8943473100662231 \n",
            "Epoch: 68 | Loss: 1.8919501304626465 \n",
            "Epoch: 69 | Loss: 1.8895591497421265 \n",
            "Epoch: 70 | Loss: 1.8871748447418213 \n",
            "Epoch: 71 | Loss: 1.8847966194152832 \n",
            "Epoch: 72 | Loss: 1.88242506980896 \n",
            "Epoch: 73 | Loss: 1.8800597190856934 \n",
            "Epoch: 74 | Loss: 1.8777005672454834 \n",
            "Epoch: 75 | Loss: 1.8753474950790405 \n",
            "Epoch: 76 | Loss: 1.8730008602142334 \n",
            "Epoch: 77 | Loss: 1.8706605434417725 \n",
            "Epoch: 78 | Loss: 1.8683263063430786 \n",
            "Epoch: 79 | Loss: 1.8659980297088623 \n",
            "Epoch: 80 | Loss: 1.8636760711669922 \n",
            "Epoch: 81 | Loss: 1.8613600730895996 \n",
            "Epoch: 82 | Loss: 1.8590502738952637 \n",
            "Epoch: 83 | Loss: 1.8567464351654053 \n",
            "Epoch: 84 | Loss: 1.854448676109314 \n",
            "Epoch: 85 | Loss: 1.8521567583084106 \n",
            "Epoch: 86 | Loss: 1.8498709201812744 \n",
            "Epoch: 87 | Loss: 1.8475914001464844 \n",
            "Epoch: 88 | Loss: 1.8453172445297241 \n",
            "Epoch: 89 | Loss: 1.84304940700531 \n",
            "Epoch: 90 | Loss: 1.8407872915267944 \n",
            "Epoch: 91 | Loss: 1.838531255722046 \n",
            "Epoch: 92 | Loss: 1.8362808227539062 \n",
            "Epoch: 93 | Loss: 1.8340364694595337 \n",
            "Epoch: 94 | Loss: 1.8317978382110596 \n",
            "Epoch: 95 | Loss: 1.8295649290084839 \n",
            "Epoch: 96 | Loss: 1.8273379802703857 \n",
            "Epoch: 97 | Loss: 1.825116515159607 \n",
            "Epoch: 98 | Loss: 1.8229012489318848 \n",
            "Epoch: 99 | Loss: 1.8206913471221924 \n",
            "Epoch: 100 | Loss: 1.818487286567688 \n",
            "Epoch: 101 | Loss: 1.8162888288497925 \n",
            "Epoch: 102 | Loss: 1.8140960931777954 \n",
            "Epoch: 103 | Loss: 1.8119089603424072 \n",
            "Epoch: 104 | Loss: 1.809727668762207 \n",
            "Epoch: 105 | Loss: 1.807551622390747 \n",
            "Epoch: 106 | Loss: 1.8053815364837646 \n",
            "Epoch: 107 | Loss: 1.8032166957855225 \n",
            "Epoch: 108 | Loss: 1.8010578155517578 \n",
            "Epoch: 109 | Loss: 1.7989041805267334 \n",
            "Epoch: 110 | Loss: 1.7967560291290283 \n",
            "Epoch: 111 | Loss: 1.7946133613586426 \n",
            "Epoch: 112 | Loss: 1.7924761772155762 \n",
            "Epoch: 113 | Loss: 1.7903445959091187 \n",
            "Epoch: 114 | Loss: 1.7882184982299805 \n",
            "Epoch: 115 | Loss: 1.786097764968872 \n",
            "Epoch: 116 | Loss: 1.783982515335083 \n",
            "Epoch: 117 | Loss: 1.7818726301193237 \n",
            "Epoch: 118 | Loss: 1.7797683477401733 \n",
            "Epoch: 119 | Loss: 1.7776689529418945 \n",
            "Epoch: 120 | Loss: 1.775575041770935 \n",
            "Epoch: 121 | Loss: 1.773486614227295 \n",
            "Epoch: 122 | Loss: 1.771403431892395 \n",
            "Epoch: 123 | Loss: 1.7693254947662354 \n",
            "Epoch: 124 | Loss: 1.7672529220581055 \n",
            "Epoch: 125 | Loss: 1.7651852369308472 \n",
            "Epoch: 126 | Loss: 1.7631232738494873 \n",
            "Epoch: 127 | Loss: 1.761066198348999 \n",
            "Epoch: 128 | Loss: 1.7590144872665405 \n",
            "Epoch: 129 | Loss: 1.7569680213928223 \n",
            "Epoch: 130 | Loss: 1.7549265623092651 \n",
            "Epoch: 131 | Loss: 1.7528903484344482 \n",
            "Epoch: 132 | Loss: 1.750859260559082 \n",
            "Epoch: 133 | Loss: 1.7488332986831665 \n",
            "Epoch: 134 | Loss: 1.7468124628067017 \n",
            "Epoch: 135 | Loss: 1.744796633720398 \n",
            "Epoch: 136 | Loss: 1.7427860498428345 \n",
            "Epoch: 137 | Loss: 1.7407803535461426 \n",
            "Epoch: 138 | Loss: 1.7387796640396118 \n",
            "Epoch: 139 | Loss: 1.7367839813232422 \n",
            "Epoch: 140 | Loss: 1.7347934246063232 \n",
            "Epoch: 141 | Loss: 1.7328077554702759 \n",
            "Epoch: 142 | Loss: 1.7308272123336792 \n",
            "Epoch: 143 | Loss: 1.7288514375686646 \n",
            "Epoch: 144 | Loss: 1.7268807888031006 \n",
            "Epoch: 145 | Loss: 1.7249149084091187 \n",
            "Epoch: 146 | Loss: 1.7229539155960083 \n",
            "Epoch: 147 | Loss: 1.7209978103637695 \n",
            "Epoch: 148 | Loss: 1.7190465927124023 \n",
            "Epoch: 149 | Loss: 1.7171005010604858 \n",
            "Epoch: 150 | Loss: 1.7151588201522827 \n",
            "Epoch: 151 | Loss: 1.7132222652435303 \n",
            "Epoch: 152 | Loss: 1.7112903594970703 \n",
            "Epoch: 153 | Loss: 1.7093634605407715 \n",
            "Epoch: 154 | Loss: 1.7074412107467651 \n",
            "Epoch: 155 | Loss: 1.7055236101150513 \n",
            "Epoch: 156 | Loss: 1.7036110162734985 \n",
            "Epoch: 157 | Loss: 1.7017028331756592 \n",
            "Epoch: 158 | Loss: 1.6997997760772705 \n",
            "Epoch: 159 | Loss: 1.6979010105133057 \n",
            "Epoch: 160 | Loss: 1.6960071325302124 \n",
            "Epoch: 161 | Loss: 1.6941179037094116 \n",
            "Epoch: 162 | Loss: 1.6922332048416138 \n",
            "Epoch: 163 | Loss: 1.690353274345398 \n",
            "Epoch: 164 | Loss: 1.6884779930114746 \n",
            "Epoch: 165 | Loss: 1.6866073608398438 \n",
            "Epoch: 166 | Loss: 1.6847411394119263 \n",
            "Epoch: 167 | Loss: 1.6828795671463013 \n",
            "Epoch: 168 | Loss: 1.6810225248336792 \n",
            "Epoch: 169 | Loss: 1.6791702508926392 \n",
            "Epoch: 170 | Loss: 1.677322506904602 \n",
            "Epoch: 171 | Loss: 1.6754789352416992 \n",
            "Epoch: 172 | Loss: 1.6736400127410889 \n",
            "Epoch: 173 | Loss: 1.6718058586120605 \n",
            "Epoch: 174 | Loss: 1.6699758768081665 \n",
            "Epoch: 175 | Loss: 1.668150544166565 \n",
            "Epoch: 176 | Loss: 1.6663293838500977 \n",
            "Epoch: 177 | Loss: 1.6645127534866333 \n",
            "Epoch: 178 | Loss: 1.6627006530761719 \n",
            "Epoch: 179 | Loss: 1.6608930826187134 \n",
            "Epoch: 180 | Loss: 1.6590896844863892 \n",
            "Epoch: 181 | Loss: 1.6572909355163574 \n",
            "Epoch: 182 | Loss: 1.6554962396621704 \n",
            "Epoch: 183 | Loss: 1.6537058353424072 \n",
            "Epoch: 184 | Loss: 1.651919960975647 \n",
            "Epoch: 185 | Loss: 1.6501384973526 \n",
            "Epoch: 186 | Loss: 1.6483612060546875 \n",
            "Epoch: 187 | Loss: 1.6465880870819092 \n",
            "Epoch: 188 | Loss: 1.6448194980621338 \n",
            "Epoch: 189 | Loss: 1.6430549621582031 \n",
            "Epoch: 190 | Loss: 1.6412948369979858 \n",
            "Epoch: 191 | Loss: 1.6395388841629028 \n",
            "Epoch: 192 | Loss: 1.637787103652954 \n",
            "Epoch: 193 | Loss: 1.6360394954681396 \n",
            "Epoch: 194 | Loss: 1.6342960596084595 \n",
            "Epoch: 195 | Loss: 1.6325569152832031 \n",
            "Epoch: 196 | Loss: 1.630821943283081 \n",
            "Epoch: 197 | Loss: 1.6290912628173828 \n",
            "Epoch: 198 | Loss: 1.6273643970489502 \n",
            "Epoch: 199 | Loss: 1.6256418228149414 \n",
            "Epoch: 200 | Loss: 1.6239233016967773 \n",
            "Epoch: 201 | Loss: 1.622209072113037 \n",
            "Epoch: 202 | Loss: 1.6204988956451416 \n",
            "Epoch: 203 | Loss: 1.6187926530838013 \n",
            "Epoch: 204 | Loss: 1.6170904636383057 \n",
            "Epoch: 205 | Loss: 1.6153923273086548 \n",
            "Epoch: 206 | Loss: 1.6136983633041382 \n",
            "Epoch: 207 | Loss: 1.6120083332061768 \n",
            "Epoch: 208 | Loss: 1.6103222370147705 \n",
            "Epoch: 209 | Loss: 1.6086403131484985 \n",
            "Epoch: 210 | Loss: 1.6069623231887817 \n",
            "Epoch: 211 | Loss: 1.6052883863449097 \n",
            "Epoch: 212 | Loss: 1.6036182641983032 \n",
            "Epoch: 213 | Loss: 1.6019519567489624 \n",
            "Epoch: 214 | Loss: 1.6002897024154663 \n",
            "Epoch: 215 | Loss: 1.598631501197815 \n",
            "Epoch: 216 | Loss: 1.5969769954681396 \n",
            "Epoch: 217 | Loss: 1.5953266620635986 \n",
            "Epoch: 218 | Loss: 1.5936800241470337 \n",
            "Epoch: 219 | Loss: 1.5920370817184448 \n",
            "Epoch: 220 | Loss: 1.5903981924057007 \n",
            "Epoch: 221 | Loss: 1.5887629985809326 \n",
            "Epoch: 222 | Loss: 1.5871319770812988 \n",
            "Epoch: 223 | Loss: 1.585504412651062 \n",
            "Epoch: 224 | Loss: 1.58388090133667 \n",
            "Epoch: 225 | Loss: 1.5822612047195435 \n",
            "Epoch: 226 | Loss: 1.5806450843811035 \n",
            "Epoch: 227 | Loss: 1.5790328979492188 \n",
            "Epoch: 228 | Loss: 1.57742440700531 \n",
            "Epoch: 229 | Loss: 1.5758198499679565 \n",
            "Epoch: 230 | Loss: 1.5742188692092896 \n",
            "Epoch: 231 | Loss: 1.572621464729309 \n",
            "Epoch: 232 | Loss: 1.5710279941558838 \n",
            "Epoch: 233 | Loss: 1.5694382190704346 \n",
            "Epoch: 234 | Loss: 1.5678520202636719 \n",
            "Epoch: 235 | Loss: 1.5662693977355957 \n",
            "Epoch: 236 | Loss: 1.5646905899047852 \n",
            "Epoch: 237 | Loss: 1.5631154775619507 \n",
            "Epoch: 238 | Loss: 1.5615439414978027 \n",
            "Epoch: 239 | Loss: 1.5599759817123413 \n",
            "Epoch: 240 | Loss: 1.5584118366241455 \n",
            "Epoch: 241 | Loss: 1.5568511486053467 \n",
            "Epoch: 242 | Loss: 1.555294156074524 \n",
            "Epoch: 243 | Loss: 1.5537406206130981 \n",
            "Epoch: 244 | Loss: 1.5521905422210693 \n",
            "Epoch: 245 | Loss: 1.5506443977355957 \n",
            "Epoch: 246 | Loss: 1.5491013526916504 \n",
            "Epoch: 247 | Loss: 1.5475621223449707 \n",
            "Epoch: 248 | Loss: 1.546026349067688 \n",
            "Epoch: 249 | Loss: 1.5444942712783813 \n",
            "Epoch: 250 | Loss: 1.5429657697677612 \n",
            "Epoch: 251 | Loss: 1.5414401292800903 \n",
            "Epoch: 252 | Loss: 1.5399187803268433 \n",
            "Epoch: 253 | Loss: 1.538400411605835 \n",
            "Epoch: 254 | Loss: 1.5368856191635132 \n",
            "Epoch: 255 | Loss: 1.5353741645812988 \n",
            "Epoch: 256 | Loss: 1.533866286277771 \n",
            "Epoch: 257 | Loss: 1.5323619842529297 \n",
            "Epoch: 258 | Loss: 1.5308607816696167 \n",
            "Epoch: 259 | Loss: 1.5293631553649902 \n",
            "Epoch: 260 | Loss: 1.5278687477111816 \n",
            "Epoch: 261 | Loss: 1.5263780355453491 \n",
            "Epoch: 262 | Loss: 1.524890661239624 \n",
            "Epoch: 263 | Loss: 1.5234065055847168 \n",
            "Epoch: 264 | Loss: 1.5219258069992065 \n",
            "Epoch: 265 | Loss: 1.5204484462738037 \n",
            "Epoch: 266 | Loss: 1.5189743041992188 \n",
            "Epoch: 267 | Loss: 1.5175033807754517 \n",
            "Epoch: 268 | Loss: 1.5160361528396606 \n",
            "Epoch: 269 | Loss: 1.5145717859268188 \n",
            "Epoch: 270 | Loss: 1.5131109952926636 \n",
            "Epoch: 271 | Loss: 1.5116534233093262 \n",
            "Epoch: 272 | Loss: 1.5101991891860962 \n",
            "Epoch: 273 | Loss: 1.508748173713684 \n",
            "Epoch: 274 | Loss: 1.5073002576828003 \n",
            "Epoch: 275 | Loss: 1.505855679512024 \n",
            "Epoch: 276 | Loss: 1.504414439201355 \n",
            "Epoch: 277 | Loss: 1.502976417541504 \n",
            "Epoch: 278 | Loss: 1.5015413761138916 \n",
            "Epoch: 279 | Loss: 1.5001096725463867 \n",
            "Epoch: 280 | Loss: 1.4986813068389893 \n",
            "Epoch: 281 | Loss: 1.4972560405731201 \n",
            "Epoch: 282 | Loss: 1.4958336353302002 \n",
            "Epoch: 283 | Loss: 1.4944148063659668 \n",
            "Epoch: 284 | Loss: 1.4929988384246826 \n",
            "Epoch: 285 | Loss: 1.4915862083435059 \n",
            "Epoch: 286 | Loss: 1.4901769161224365 \n",
            "Epoch: 287 | Loss: 1.4887701272964478 \n",
            "Epoch: 288 | Loss: 1.487367033958435 \n",
            "Epoch: 289 | Loss: 1.4859668016433716 \n",
            "Epoch: 290 | Loss: 1.4845696687698364 \n",
            "Epoch: 291 | Loss: 1.48317551612854 \n",
            "Epoch: 292 | Loss: 1.4817843437194824 \n",
            "Epoch: 293 | Loss: 1.4803963899612427 \n",
            "Epoch: 294 | Loss: 1.4790117740631104 \n",
            "Epoch: 295 | Loss: 1.4776296615600586 \n",
            "Epoch: 296 | Loss: 1.4762511253356934 \n",
            "Epoch: 297 | Loss: 1.4748752117156982 \n",
            "Epoch: 298 | Loss: 1.4735026359558105 \n",
            "Epoch: 299 | Loss: 1.4721330404281616 \n",
            "Epoch: 300 | Loss: 1.4707660675048828 \n",
            "Epoch: 301 | Loss: 1.4694023132324219 \n",
            "Epoch: 302 | Loss: 1.4680416584014893 \n",
            "Epoch: 303 | Loss: 1.4666837453842163 \n",
            "Epoch: 304 | Loss: 1.4653288125991821 \n",
            "Epoch: 305 | Loss: 1.4639772176742554 \n",
            "Epoch: 306 | Loss: 1.4626280069351196 \n",
            "Epoch: 307 | Loss: 1.4612820148468018 \n",
            "Epoch: 308 | Loss: 1.4599390029907227 \n",
            "Epoch: 309 | Loss: 1.4585989713668823 \n",
            "Epoch: 310 | Loss: 1.4572614431381226 \n",
            "Epoch: 311 | Loss: 1.4559271335601807 \n",
            "Epoch: 312 | Loss: 1.4545955657958984 \n",
            "Epoch: 313 | Loss: 1.453266978263855 \n",
            "Epoch: 314 | Loss: 1.4519412517547607 \n",
            "Epoch: 315 | Loss: 1.4506185054779053 \n",
            "Epoch: 316 | Loss: 1.44929838180542 \n",
            "Epoch: 317 | Loss: 1.447981357574463 \n",
            "Epoch: 318 | Loss: 1.446666955947876 \n",
            "Epoch: 319 | Loss: 1.4453555345535278 \n",
            "Epoch: 320 | Loss: 1.4440467357635498 \n",
            "Epoch: 321 | Loss: 1.4427409172058105 \n",
            "Epoch: 322 | Loss: 1.4414377212524414 \n",
            "Epoch: 323 | Loss: 1.440137505531311 \n",
            "Epoch: 324 | Loss: 1.4388399124145508 \n",
            "Epoch: 325 | Loss: 1.4375455379486084 \n",
            "Epoch: 326 | Loss: 1.4362534284591675 \n",
            "Epoch: 327 | Loss: 1.4349644184112549 \n",
            "Epoch: 328 | Loss: 1.4336777925491333 \n",
            "Epoch: 329 | Loss: 1.432394027709961 \n",
            "Epoch: 330 | Loss: 1.4311130046844482 \n",
            "Epoch: 331 | Loss: 1.4298349618911743 \n",
            "Epoch: 332 | Loss: 1.428559422492981 \n",
            "Epoch: 333 | Loss: 1.4272866249084473 \n",
            "Epoch: 334 | Loss: 1.4260165691375732 \n",
            "Epoch: 335 | Loss: 1.4247491359710693 \n",
            "Epoch: 336 | Loss: 1.4234845638275146 \n",
            "Epoch: 337 | Loss: 1.42222261428833 \n",
            "Epoch: 338 | Loss: 1.420963168144226 \n",
            "Epoch: 339 | Loss: 1.4197064638137817 \n",
            "Epoch: 340 | Loss: 1.4184521436691284 \n",
            "Epoch: 341 | Loss: 1.417201042175293 \n",
            "Epoch: 342 | Loss: 1.415952205657959 \n",
            "Epoch: 343 | Loss: 1.4147058725357056 \n",
            "Epoch: 344 | Loss: 1.4134624004364014 \n",
            "Epoch: 345 | Loss: 1.4122215509414673 \n",
            "Epoch: 346 | Loss: 1.4109833240509033 \n",
            "Epoch: 347 | Loss: 1.4097473621368408 \n",
            "Epoch: 348 | Loss: 1.4085144996643066 \n",
            "Epoch: 349 | Loss: 1.4072836637496948 \n",
            "Epoch: 350 | Loss: 1.4060558080673218 \n",
            "Epoch: 351 | Loss: 1.4048304557800293 \n",
            "Epoch: 352 | Loss: 1.4036076068878174 \n",
            "Epoch: 353 | Loss: 1.4023874998092651 \n",
            "Epoch: 354 | Loss: 1.4011694192886353 \n",
            "Epoch: 355 | Loss: 1.3999544382095337 \n",
            "Epoch: 356 | Loss: 1.3987418413162231 \n",
            "Epoch: 357 | Loss: 1.3975313901901245 \n",
            "Epoch: 358 | Loss: 1.3963239192962646 \n",
            "Epoch: 359 | Loss: 1.3951187133789062 \n",
            "Epoch: 360 | Loss: 1.3939162492752075 \n",
            "Epoch: 361 | Loss: 1.3927160501480103 \n",
            "Epoch: 362 | Loss: 1.391518473625183 \n",
            "Epoch: 363 | Loss: 1.3903229236602783 \n",
            "Epoch: 364 | Loss: 1.3891304731369019 \n",
            "Epoch: 365 | Loss: 1.3879401683807373 \n",
            "Epoch: 366 | Loss: 1.3867522478103638 \n",
            "Epoch: 367 | Loss: 1.3855669498443604 \n",
            "Epoch: 368 | Loss: 1.384384036064148 \n",
            "Epoch: 369 | Loss: 1.3832035064697266 \n",
            "Epoch: 370 | Loss: 1.3820254802703857 \n",
            "Epoch: 371 | Loss: 1.3808499574661255 \n",
            "Epoch: 372 | Loss: 1.3796764612197876 \n",
            "Epoch: 373 | Loss: 1.3785059452056885 \n",
            "Epoch: 374 | Loss: 1.3773374557495117 \n",
            "Epoch: 375 | Loss: 1.376171350479126 \n",
            "Epoch: 376 | Loss: 1.3750077486038208 \n",
            "Epoch: 377 | Loss: 1.3738465309143066 \n",
            "Epoch: 378 | Loss: 1.372687578201294 \n",
            "Epoch: 379 | Loss: 1.3715310096740723 \n",
            "Epoch: 380 | Loss: 1.370376706123352 \n",
            "Epoch: 381 | Loss: 1.3692249059677124 \n",
            "Epoch: 382 | Loss: 1.3680752515792847 \n",
            "Epoch: 383 | Loss: 1.366928219795227 \n",
            "Epoch: 384 | Loss: 1.3657832145690918 \n",
            "Epoch: 385 | Loss: 1.3646408319473267 \n",
            "Epoch: 386 | Loss: 1.3635005950927734 \n",
            "Epoch: 387 | Loss: 1.3623628616333008 \n",
            "Epoch: 388 | Loss: 1.3612271547317505 \n",
            "Epoch: 389 | Loss: 1.3600934743881226 \n",
            "Epoch: 390 | Loss: 1.3589625358581543 \n",
            "Epoch: 391 | Loss: 1.3578338623046875 \n",
            "Epoch: 392 | Loss: 1.3567074537277222 \n",
            "Epoch: 393 | Loss: 1.3555831909179688 \n",
            "Epoch: 394 | Loss: 1.3544609546661377 \n",
            "Epoch: 395 | Loss: 1.3533413410186768 \n",
            "Epoch: 396 | Loss: 1.3522238731384277 \n",
            "Epoch: 397 | Loss: 1.3511085510253906 \n",
            "Epoch: 398 | Loss: 1.349995732307434 \n",
            "Epoch: 399 | Loss: 1.3488847017288208 \n",
            "Epoch: 400 | Loss: 1.3477764129638672 \n",
            "Epoch: 401 | Loss: 1.3466699123382568 \n",
            "Epoch: 402 | Loss: 1.345565676689148 \n",
            "Epoch: 403 | Loss: 1.3444639444351196 \n",
            "Epoch: 404 | Loss: 1.3433642387390137 \n",
            "Epoch: 405 | Loss: 1.3422666788101196 \n",
            "Epoch: 406 | Loss: 1.3411712646484375 \n",
            "Epoch: 407 | Loss: 1.3400782346725464 \n",
            "Epoch: 408 | Loss: 1.338987112045288 \n",
            "Epoch: 409 | Loss: 1.3378982543945312 \n",
            "Epoch: 410 | Loss: 1.3368116617202759 \n",
            "Epoch: 411 | Loss: 1.3357270956039429 \n",
            "Epoch: 412 | Loss: 1.3346446752548218 \n",
            "Epoch: 413 | Loss: 1.3335644006729126 \n",
            "Epoch: 414 | Loss: 1.3324863910675049 \n",
            "Epoch: 415 | Loss: 1.3314101696014404 \n",
            "Epoch: 416 | Loss: 1.3303362131118774 \n",
            "Epoch: 417 | Loss: 1.3292644023895264 \n",
            "Epoch: 418 | Loss: 1.3281947374343872 \n",
            "Epoch: 419 | Loss: 1.32712721824646 \n",
            "Epoch: 420 | Loss: 1.3260616064071655 \n",
            "Epoch: 421 | Loss: 1.3249984979629517 \n",
            "Epoch: 422 | Loss: 1.323937177658081 \n",
            "Epoch: 423 | Loss: 1.3228778839111328 \n",
            "Epoch: 424 | Loss: 1.3218204975128174 \n",
            "Epoch: 425 | Loss: 1.320765495300293 \n",
            "Epoch: 426 | Loss: 1.319712519645691 \n",
            "Epoch: 427 | Loss: 1.3186616897583008 \n",
            "Epoch: 428 | Loss: 1.317612648010254 \n",
            "Epoch: 429 | Loss: 1.316565752029419 \n",
            "Epoch: 430 | Loss: 1.3155208826065063 \n",
            "Epoch: 431 | Loss: 1.3144781589508057 \n",
            "Epoch: 432 | Loss: 1.3134373426437378 \n",
            "Epoch: 433 | Loss: 1.3123984336853027 \n",
            "Epoch: 434 | Loss: 1.3113617897033691 \n",
            "Epoch: 435 | Loss: 1.3103269338607788 \n",
            "Epoch: 436 | Loss: 1.3092941045761108 \n",
            "Epoch: 437 | Loss: 1.3082633018493652 \n",
            "Epoch: 438 | Loss: 1.307234525680542 \n",
            "Epoch: 439 | Loss: 1.3062077760696411 \n",
            "Epoch: 440 | Loss: 1.3051828145980835 \n",
            "Epoch: 441 | Loss: 1.3041601181030273 \n",
            "Epoch: 442 | Loss: 1.3031394481658936 \n",
            "Epoch: 443 | Loss: 1.3021204471588135 \n",
            "Epoch: 444 | Loss: 1.3011033535003662 \n",
            "Epoch: 445 | Loss: 1.3000884056091309 \n",
            "Epoch: 446 | Loss: 1.2990751266479492 \n",
            "Epoch: 447 | Loss: 1.2980642318725586 \n",
            "Epoch: 448 | Loss: 1.2970550060272217 \n",
            "Epoch: 449 | Loss: 1.2960476875305176 \n",
            "Epoch: 450 | Loss: 1.2950422763824463 \n",
            "Epoch: 451 | Loss: 1.294039011001587 \n",
            "Epoch: 452 | Loss: 1.2930374145507812 \n",
            "Epoch: 453 | Loss: 1.2920377254486084 \n",
            "Epoch: 454 | Loss: 1.2910401821136475 \n",
            "Epoch: 455 | Loss: 1.2900441884994507 \n",
            "Epoch: 456 | Loss: 1.289050579071045 \n",
            "Epoch: 457 | Loss: 1.2880584001541138 \n",
            "Epoch: 458 | Loss: 1.2870683670043945 \n",
            "Epoch: 459 | Loss: 1.286080002784729 \n",
            "Epoch: 460 | Loss: 1.2850935459136963 \n",
            "Epoch: 461 | Loss: 1.2841089963912964 \n",
            "Epoch: 462 | Loss: 1.2831264734268188 \n",
            "Epoch: 463 | Loss: 1.282145619392395 \n",
            "Epoch: 464 | Loss: 1.2811665534973145 \n",
            "Epoch: 465 | Loss: 1.2801893949508667 \n",
            "Epoch: 466 | Loss: 1.2792141437530518 \n",
            "Epoch: 467 | Loss: 1.2782409191131592 \n",
            "Epoch: 468 | Loss: 1.2772691249847412 \n",
            "Epoch: 469 | Loss: 1.276299238204956 \n",
            "Epoch: 470 | Loss: 1.2753312587738037 \n",
            "Epoch: 471 | Loss: 1.2743651866912842 \n",
            "Epoch: 472 | Loss: 1.273400902748108 \n",
            "Epoch: 473 | Loss: 1.272438406944275 \n",
            "Epoch: 474 | Loss: 1.2714776992797852 \n",
            "Epoch: 475 | Loss: 1.2705185413360596 \n",
            "Epoch: 476 | Loss: 1.2695612907409668 \n",
            "Epoch: 477 | Loss: 1.2686060667037964 \n",
            "Epoch: 478 | Loss: 1.2676522731781006 \n",
            "Epoch: 479 | Loss: 1.2667005062103271 \n",
            "Epoch: 480 | Loss: 1.2657502889633179 \n",
            "Epoch: 481 | Loss: 1.264802098274231 \n",
            "Epoch: 482 | Loss: 1.2638555765151978 \n",
            "Epoch: 483 | Loss: 1.2629108428955078 \n",
            "Epoch: 484 | Loss: 1.2619677782058716 \n",
            "Epoch: 485 | Loss: 1.261026382446289 \n",
            "Epoch: 486 | Loss: 1.2600871324539185 \n",
            "Epoch: 487 | Loss: 1.2591493129730225 \n",
            "Epoch: 488 | Loss: 1.2582131624221802 \n",
            "Epoch: 489 | Loss: 1.2572788000106812 \n",
            "Epoch: 490 | Loss: 1.2563459873199463 \n",
            "Epoch: 491 | Loss: 1.2554148435592651 \n",
            "Epoch: 492 | Loss: 1.2544857263565063 \n",
            "Epoch: 493 | Loss: 1.2535583972930908 \n",
            "Epoch: 494 | Loss: 1.2526326179504395 \n",
            "Epoch: 495 | Loss: 1.2517083883285522 \n",
            "Epoch: 496 | Loss: 1.2507861852645874 \n",
            "Epoch: 497 | Loss: 1.2498652935028076 \n",
            "Epoch: 498 | Loss: 1.2489463090896606 \n",
            "Epoch: 499 | Loss: 1.2480288743972778 \n",
            "prediction 0.20109350979328156 0.20109350979328156\n",
            "Prediction (after training) 4 False\n",
            "prediction 0.9968332648277283 0.9968332648277283\n",
            "Prediction (after training) 4 True\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}