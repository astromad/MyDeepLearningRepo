{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProductClassificationTensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1P3LPpDYFee9ExIEClrv5xkbKq7kSh1a5",
      "authorship_tag": "ABX9TyPnUMT5ByQ4wXV8kTEJstRk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astromad/MyDeepLearningRepo/blob/master/ProductClassificationTensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkHZ-jjwNx0t"
      },
      "source": [
        "!rm -rf Classification_cache_TF\n",
        "!rm -rf results_TF\n",
        "!rm -rf logs_TF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4JgF8oNvoUl"
      },
      "source": [
        "!pip install --upgrade tf-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn-T04pOfqUL"
      },
      "source": [
        "#!rm -rf Classification_cache\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN3fn6UJOA-x"
      },
      "source": [
        "#!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mDOS-p0eKd2"
      },
      "source": [
        "!pip install --upgrade git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFq-DEEqOERC"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/My Drive/ColabData/Amazon.csv\",\n",
        "                encoding=\"ISO-8859-1\", error_bad_lines=False)\n",
        "\n",
        "data = df[['category', 'label_title', 'label_description']]\n",
        "data.dropna(subset=['category'], inplace=True)\n",
        "print(data.head(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_MDIwMrdY44"
      },
      "source": [
        "# Remove rows if category is null\n",
        "data.dropna(subset=['category'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlpCaw2rOH-i"
      },
      "source": [
        "encode_dict={}\n",
        "def encode_label(x):\n",
        "    if x not in encode_dict.keys():\n",
        "        encode_dict[x]=len(encode_dict)\n",
        "    return encode_dict[x]\n",
        "\n",
        "data['encoded_category'] = data['category'].apply(lambda x: encode_label(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU7InCkWOMwy"
      },
      "source": [
        "newData=pd.DataFrame()\n",
        "newData['desc']=data['label_title'] +' '+ data['label_description'] \n",
        "newData['encoded_category']=data['encoded_category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YW92EO8dwOU"
      },
      "source": [
        "# drop any rows with description is null\n",
        "newData.dropna(subset=['desc'], inplace=True)\n",
        "nan_rows = newData[newData.isnull().T.any()]\n",
        "print(nan_rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru7fZLXSd7xI"
      },
      "source": [
        "newData.loc[20,'desc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWegBGCkeLDn"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U_1LHAweP-Q"
      },
      "source": [
        "# Preprocessing on description text data, remove stop words, remove spaces, lowercase\n",
        "# note: we are not lemmatize as Bert will take care of it\n",
        "newData['desc']=newData.desc.str.replace(\"[^\\w\\s]\", \"\").str.lower()\n",
        "#newData['desc']=newData.desc.str.replace('\\d+', '')\n",
        "#newData['desc']=newData['desc'].apply(lambda x: [item for item in x.split() if item not in stop])\n",
        "newData['desc']=newData['desc'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WDDPDCReSG-"
      },
      "source": [
        "newData.loc[20,'desc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HupS6VvgkeWp"
      },
      "source": [
        "from future.utils import iteritems\n",
        "label2idx = {t: i for i, t in enumerate(encode_dict)}\n",
        "idx2label = {v: k for k, v in iteritems(label2idx)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unO0K-ZgOUii"
      },
      "source": [
        "#newData.dropna(subset=['desc'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoVqVCcqOedB"
      },
      "source": [
        "# nan_rows = newData[newData.isnull().T.any()]\n",
        "# print(nan_rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPlXYoVVOQS8"
      },
      "source": [
        "print(newData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kYkLwc_OhtH"
      },
      "source": [
        "ClassMax=newData['encoded_category'].max()\n",
        "print(ClassMax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhegJzgzOlWZ"
      },
      "source": [
        "train_size = 0.8\n",
        "train_dataset=newData.sample(frac=train_size,random_state=200)\n",
        "test_dataset=newData.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(newData.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bz7yVP7gzF_"
      },
      "source": [
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    #BertTokenizer\n",
        ")\n",
        "model_args = dict()\n",
        "model_args['model_name'] = 'bert-base-uncased' \n",
        "model_args['cache_dir'] = \"Classification_cache_TF/\"\n",
        "model_args['do_basic_tokenize'] = False\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    model_args['model_name'],\n",
        "    # num_labels=num_labels,\n",
        "    # id2label=label_map,\n",
        "    # label2id={label: i for i, label in enumerate(labels)},\n",
        "    cache_dir=model_args['cache_dir'],\n",
        "    return_dict=True,\n",
        "    num_labels=ClassMax+1\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_args['model_name'],\n",
        "    cache_dir=model_args['cache_dir'],\n",
        "    is_pretokenized=model_args['do_basic_tokenize'],\n",
        "    do_basic_tokenize = model_args['do_basic_tokenize']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kD4ylywYWp4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zg_FrVsY0yd"
      },
      "source": [
        "import torch\n",
        "import re\n",
        "class TorchClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,dataset,max_len):\n",
        "        self.len = len(dataset)\n",
        "        self.data = dataset\n",
        "        self.max_len=max_len\n",
        "    def __getitem__(self, idx):\n",
        "        #item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        description = str(self.data.desc[idx])\n",
        "        #description = \" \".join(description.split())\n",
        "        #print(description)\n",
        "        description = description[:self.max_len]\n",
        "        # description = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', description)\n",
        "        # description = \" \".join(description.split())\n",
        "        #description = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', description)\n",
        "        #print(description)\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            description,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            #pad_to_max_length=True,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        item ={}\n",
        "        item['input_ids']=torch.tensor(inputs['input_ids'], dtype=torch.long)\n",
        "        item['token_type_ids']=torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n",
        "        item['attention_mask']=torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
        "        item['labels'] = torch.tensor(self.data.encoded_category[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLeb2jT1gO4i"
      },
      "source": [
        "#[np.put(np.zeros(ClassMax+1),y,1) for y in test_dataset['encoded_category']]\n",
        "def one_hot(i):\n",
        "    a = np.zeros(ClassMax+1)\n",
        "    a[i] = 1\n",
        "    return a\n",
        "#[one_hot(y) for y in test_dataset['encoded_category']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFE924XVY28_"
      },
      "source": [
        "import numpy as np\n",
        "def TFClassificationDataset(dataframe,MAX_LEN):\n",
        "        # description = str(train_dataset.desc[idx])\n",
        "        # #description = \" \".join(description.split())\n",
        "        # #print(description)\n",
        "        # description = description[:self.max_len]\n",
        "        # description = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', description)\n",
        "        # description = \" \".join(description.split())\n",
        "        description =dataframe['desc'].tolist()        \n",
        "        #description = [re.sub('\\W+',' ', x) for x in description]\n",
        "        #print(description)\n",
        "        inputs = tokenizer.batch_encode_plus(\n",
        "            description,\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            pad_to_max_length=True,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        arr = np.zeros(10)\n",
        "        np.put(arr, 5, 1)\n",
        "        return inputs['input_ids'],inputs['attention_mask'],inputs['token_type_ids'],[[x] for x in dataframe['encoded_category']]\n",
        "        #return inputs['input_ids'],inputs['attention_mask'],inputs['token_type_ids'],[one_hot(x) for x in dataframe['encoded_category']]\n",
        "\n",
        "        # print(inputs['input_ids'])\n",
        "        # print(tf.data.Dataset.from_tensor_slices(inputs['input_ids']))\n",
        "        # return {\n",
        "        #     tf.data.Dataset.from_tensor_slices(inputs['input_ids']),\n",
        "        #     tf.data.Dataset.from_tensor_slices(inputs['attention_mask']),\n",
        "        #     tf.data.Dataset.from_tensor_slices(inputs['token_type_ids']),\n",
        "        #     tf.data.Dataset.from_tensor_slices([[x] for x in dataframe['encoded_category']])\n",
        "        # }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLnM_gMdgRKg"
      },
      "source": [
        "MAX_LEN=256\n",
        "LEARNING_RATE = 3e-03"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbX4lTS3Rrgz"
      },
      "source": [
        "def example_to_features(input_ids,attention_masks,token_type_ids,y):\n",
        "  return {\"input_ids\": input_ids,\n",
        "          \"attention_mask\": attention_masks,\n",
        "          \"token_type_ids\": token_type_ids},y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSYs8kXFO0os"
      },
      "source": [
        "import tensorflow as tf\n",
        "def createDataset(framework='pt'):\n",
        "  if framework=='pt':\n",
        "    train_ds = TorchClassificationDataset(train_dataset,MAX_LEN)\n",
        "    test_ds= TorchClassificationDataset(test_dataset,MAX_LEN)\n",
        "  else:\n",
        "    input_ids_train,attention_masks_train,token_ids_train,label_ids_train= TFClassificationDataset(train_dataset,MAX_LEN)\n",
        "    input_ids_test,attention_masks_test,token_ids_test,label_ids_test= TFClassificationDataset(test_dataset,MAX_LEN)\n",
        "\n",
        "    # train_ds= TFClassificationDataset(train_dataset.loc[0:1],MAX_LEN)\n",
        "\n",
        "    # print(input_ids_train)\n",
        "    # print(attention_masks_train)\n",
        "    # print(token_ids_train)\n",
        "    # print(label_ids_train)\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((input_ids_train,attention_masks_train,token_ids_train,label_ids_train)).map(example_to_features)\n",
        "    test_ds=tf.data.Dataset.from_tensor_slices((input_ids_test,attention_masks_test,token_ids_test,label_ids_test)).map(example_to_features)\n",
        "  return train_ds,test_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCUEc80-YAMt"
      },
      "source": [
        "train_ds,test_ds = createDataset('tf')\n",
        "print('One record of Training dataset')\n",
        "print(train_dataset.loc[1,'desc'])\n",
        "print('----')\n",
        "for x,y in train_ds.take(1):\n",
        "  print(x)\n",
        "  print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKQKmLpQbSVs"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWTSnVVLYuzw"
      },
      "source": [
        "from transformers import (\n",
        "    TFAutoModelForSequenceClassification,\n",
        "    #BertForSequenceClassification,\n",
        "    TFTrainer,\n",
        "    TFTrainingArguments\n",
        ")\n",
        "\n",
        "training_argsTF = TFTrainingArguments(\n",
        "    output_dir='./results_TF',          \n",
        "    num_train_epochs=20,              \n",
        "    per_device_train_batch_size=8,  \n",
        "    per_device_eval_batch_size=8,   \n",
        "    warmup_steps=100,                \n",
        "    weight_decay=0.01,               \n",
        "    logging_dir='./logs_TF',            \n",
        "    logging_steps=3,\n",
        "    #debug=True\n",
        "    #evaluate_during_training=True,\n",
        "    #learning_rate=LEARNING_RATE\n",
        ")\n",
        "with training_argsTF.strategy.scope():\n",
        "  modelTF = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "    model_args['model_name'],\n",
        "    config=config,\n",
        "    cache_dir=model_args['cache_dir'],\n",
        "  )\n",
        "trainerTF = TFTrainer(\n",
        "    model=modelTF,                         \n",
        "    args=training_argsTF,                  \n",
        "    train_dataset=train_ds,        \n",
        "    eval_dataset=test_ds,\n",
        "    compute_metrics=compute_metrics,  \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1m6GrncZdL4"
      },
      "source": [
        "trainerTF.train() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGe1VYColAZ-"
      },
      "source": [
        "trainerTF.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIKHuPV0xngi"
      },
      "source": [
        "predictions, label_ids, metrics = trainerTF.predict(test_ds)\n",
        "for key, value in metrics.items():\n",
        "    print( key, value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiQ0V4k2lDK-"
      },
      "source": [
        "inputs = tokenizer(\"Da-Lite Stand Master I - Cart for projector Projection Carts - Stand Master I Features The height of both the upper and lower shelves\", return_tensors=\"tf\")\n",
        "print(inputs)\n",
        "labels = tf.constant([76])\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvrvpv6DlyRu"
      },
      "source": [
        "import numpy as np\n",
        "outputs = modelTF(inputs, labels=labels)\n",
        "print(outputs.loss)\n",
        "pred=np.argmax(outputs.logits)\n",
        "print('prediction=',pred,idx2label[76])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e148hGFsvmiS"
      },
      "source": [
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "# modelTF.compile(optimizer=optimizer, loss=loss)\n",
        "# #n_steps = train_ds.shape[0] \n",
        "# modelTF.fit(train_ds,epochs=1, steps_per_epoch=256)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}