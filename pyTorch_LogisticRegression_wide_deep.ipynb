{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyTorch_LogisticRegression_wide_deep.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPei4QQxJmkWuPmCJ34o+6o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astromad/MyDeepLearningRepo/blob/master/pyTorch_LogisticRegression_wide_deep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0NNEIaFDSYZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b24489c-bebd-4954-cb95-909783f828f4"
      },
      "source": [
        "from torch import nn,optim,from_numpy\n",
        "import torch\n",
        "from torch import tensor,sigmoid\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "xy = np.loadtxt('./diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
        "x_data = from_numpy(xy[:,0:-1])\n",
        "y_data = from_numpy(xy[:,[-1]])\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()        \n",
        "        self.l1 = nn.Linear(8, 6)\n",
        "        self.l2 = nn.Linear(6, 4)\n",
        "        self.l3 = nn.Linear(4, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        out1 = sigmoid(self.l1(x))\n",
        "        out2 = sigmoid(self.l2(out1))\n",
        "        y_pred = sigmoid(self.l3(out2))\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.BCELoss(reduction='mean')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 2) Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
        "    print(f'Epoch: {epoch + 1}/100 | Loss: {loss.item():.4f}')\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# After training\n",
        "#hour_var = tensor([[1.0]])\n",
        "#y_pred = model(hour_var)\n",
        "#print('prediction',y_pred.item(),model(hour_var).data.item())\n",
        "#print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item()> 0.5)\n",
        "#hour_var = tensor([[7.0]])\n",
        "#y_pred = model(hour_var)\n",
        "#print('prediction',y_pred.item(),model(hour_var).data[0][0].item())\n",
        "#print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item()> 0.5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X's shape: torch.Size([759, 8]) | Y's shape: torch.Size([759, 1])\n",
            "Epoch: 0 | Loss: 0.8874043822288513 \n",
            "Epoch: 1/100 | Loss: 0.8874\n",
            "Epoch: 1 | Loss: 0.8848235011100769 \n",
            "Epoch: 2/100 | Loss: 0.8848\n",
            "Epoch: 2 | Loss: 0.8822691440582275 \n",
            "Epoch: 3/100 | Loss: 0.8823\n",
            "Epoch: 3 | Loss: 0.8797402381896973 \n",
            "Epoch: 4/100 | Loss: 0.8797\n",
            "Epoch: 4 | Loss: 0.87723708152771 \n",
            "Epoch: 5/100 | Loss: 0.8772\n",
            "Epoch: 5 | Loss: 0.874758780002594 \n",
            "Epoch: 6/100 | Loss: 0.8748\n",
            "Epoch: 6 | Loss: 0.872305691242218 \n",
            "Epoch: 7/100 | Loss: 0.8723\n",
            "Epoch: 7 | Loss: 0.8698774576187134 \n",
            "Epoch: 8/100 | Loss: 0.8699\n",
            "Epoch: 8 | Loss: 0.8674740195274353 \n",
            "Epoch: 9/100 | Loss: 0.8675\n",
            "Epoch: 9 | Loss: 0.8650947213172913 \n",
            "Epoch: 10/100 | Loss: 0.8651\n",
            "Epoch: 10 | Loss: 0.8627399802207947 \n",
            "Epoch: 11/100 | Loss: 0.8627\n",
            "Epoch: 11 | Loss: 0.860409140586853 \n",
            "Epoch: 12/100 | Loss: 0.8604\n",
            "Epoch: 12 | Loss: 0.8581018447875977 \n",
            "Epoch: 13/100 | Loss: 0.8581\n",
            "Epoch: 13 | Loss: 0.8558186888694763 \n",
            "Epoch: 14/100 | Loss: 0.8558\n",
            "Epoch: 14 | Loss: 0.8535586595535278 \n",
            "Epoch: 15/100 | Loss: 0.8536\n",
            "Epoch: 15 | Loss: 0.8513218760490417 \n",
            "Epoch: 16/100 | Loss: 0.8513\n",
            "Epoch: 16 | Loss: 0.8491078019142151 \n",
            "Epoch: 17/100 | Loss: 0.8491\n",
            "Epoch: 17 | Loss: 0.8469168543815613 \n",
            "Epoch: 18/100 | Loss: 0.8469\n",
            "Epoch: 18 | Loss: 0.8447484374046326 \n",
            "Epoch: 19/100 | Loss: 0.8447\n",
            "Epoch: 19 | Loss: 0.8426023721694946 \n",
            "Epoch: 20/100 | Loss: 0.8426\n",
            "Epoch: 20 | Loss: 0.8404783606529236 \n",
            "Epoch: 21/100 | Loss: 0.8405\n",
            "Epoch: 21 | Loss: 0.8383762240409851 \n",
            "Epoch: 22/100 | Loss: 0.8384\n",
            "Epoch: 22 | Loss: 0.8362961411476135 \n",
            "Epoch: 23/100 | Loss: 0.8363\n",
            "Epoch: 23 | Loss: 0.8342375159263611 \n",
            "Epoch: 24/100 | Loss: 0.8342\n",
            "Epoch: 24 | Loss: 0.8322002291679382 \n",
            "Epoch: 25/100 | Loss: 0.8322\n",
            "Epoch: 25 | Loss: 0.8301841020584106 \n",
            "Epoch: 26/100 | Loss: 0.8302\n",
            "Epoch: 26 | Loss: 0.828188955783844 \n",
            "Epoch: 27/100 | Loss: 0.8282\n",
            "Epoch: 27 | Loss: 0.8262146711349487 \n",
            "Epoch: 28/100 | Loss: 0.8262\n",
            "Epoch: 28 | Loss: 0.824260950088501 \n",
            "Epoch: 29/100 | Loss: 0.8243\n",
            "Epoch: 29 | Loss: 0.8223275542259216 \n",
            "Epoch: 30/100 | Loss: 0.8223\n",
            "Epoch: 30 | Loss: 0.8204147815704346 \n",
            "Epoch: 31/100 | Loss: 0.8204\n",
            "Epoch: 31 | Loss: 0.8185215592384338 \n",
            "Epoch: 32/100 | Loss: 0.8185\n",
            "Epoch: 32 | Loss: 0.8166483640670776 \n",
            "Epoch: 33/100 | Loss: 0.8166\n",
            "Epoch: 33 | Loss: 0.8147949576377869 \n",
            "Epoch: 34/100 | Loss: 0.8148\n",
            "Epoch: 34 | Loss: 0.8129606246948242 \n",
            "Epoch: 35/100 | Loss: 0.8130\n",
            "Epoch: 35 | Loss: 0.8111460208892822 \n",
            "Epoch: 36/100 | Loss: 0.8111\n",
            "Epoch: 36 | Loss: 0.8093503713607788 \n",
            "Epoch: 37/100 | Loss: 0.8094\n",
            "Epoch: 37 | Loss: 0.807573676109314 \n",
            "Epoch: 38/100 | Loss: 0.8076\n",
            "Epoch: 38 | Loss: 0.805815577507019 \n",
            "Epoch: 39/100 | Loss: 0.8058\n",
            "Epoch: 39 | Loss: 0.8040764331817627 \n",
            "Epoch: 40/100 | Loss: 0.8041\n",
            "Epoch: 40 | Loss: 0.8023551106452942 \n",
            "Epoch: 41/100 | Loss: 0.8024\n",
            "Epoch: 41 | Loss: 0.8006520867347717 \n",
            "Epoch: 42/100 | Loss: 0.8007\n",
            "Epoch: 42 | Loss: 0.7989675402641296 \n",
            "Epoch: 43/100 | Loss: 0.7990\n",
            "Epoch: 43 | Loss: 0.7973006367683411 \n",
            "Epoch: 44/100 | Loss: 0.7973\n",
            "Epoch: 44 | Loss: 0.795651376247406 \n",
            "Epoch: 45/100 | Loss: 0.7957\n",
            "Epoch: 45 | Loss: 0.7940195202827454 \n",
            "Epoch: 46/100 | Loss: 0.7940\n",
            "Epoch: 46 | Loss: 0.7924051880836487 \n",
            "Epoch: 47/100 | Loss: 0.7924\n",
            "Epoch: 47 | Loss: 0.7908079028129578 \n",
            "Epoch: 48/100 | Loss: 0.7908\n",
            "Epoch: 48 | Loss: 0.7892277240753174 \n",
            "Epoch: 49/100 | Loss: 0.7892\n",
            "Epoch: 49 | Loss: 0.7876642942428589 \n",
            "Epoch: 50/100 | Loss: 0.7877\n",
            "Epoch: 50 | Loss: 0.7861173748970032 \n",
            "Epoch: 51/100 | Loss: 0.7861\n",
            "Epoch: 51 | Loss: 0.7845870852470398 \n",
            "Epoch: 52/100 | Loss: 0.7846\n",
            "Epoch: 52 | Loss: 0.7830734252929688 \n",
            "Epoch: 53/100 | Loss: 0.7831\n",
            "Epoch: 53 | Loss: 0.7815757393836975 \n",
            "Epoch: 54/100 | Loss: 0.7816\n",
            "Epoch: 54 | Loss: 0.780093789100647 \n",
            "Epoch: 55/100 | Loss: 0.7801\n",
            "Epoch: 55 | Loss: 0.7786281704902649 \n",
            "Epoch: 56/100 | Loss: 0.7786\n",
            "Epoch: 56 | Loss: 0.777178168296814 \n",
            "Epoch: 57/100 | Loss: 0.7772\n",
            "Epoch: 57 | Loss: 0.7757434248924255 \n",
            "Epoch: 58/100 | Loss: 0.7757\n",
            "Epoch: 58 | Loss: 0.7743242383003235 \n",
            "Epoch: 59/100 | Loss: 0.7743\n",
            "Epoch: 59 | Loss: 0.7729199528694153 \n",
            "Epoch: 60/100 | Loss: 0.7729\n",
            "Epoch: 60 | Loss: 0.7715311646461487 \n",
            "Epoch: 61/100 | Loss: 0.7715\n",
            "Epoch: 61 | Loss: 0.7701573371887207 \n",
            "Epoch: 62/100 | Loss: 0.7702\n",
            "Epoch: 62 | Loss: 0.7687981128692627 \n",
            "Epoch: 63/100 | Loss: 0.7688\n",
            "Epoch: 63 | Loss: 0.7674532532691956 \n",
            "Epoch: 64/100 | Loss: 0.7675\n",
            "Epoch: 64 | Loss: 0.7661234736442566 \n",
            "Epoch: 65/100 | Loss: 0.7661\n",
            "Epoch: 65 | Loss: 0.764807403087616 \n",
            "Epoch: 66/100 | Loss: 0.7648\n",
            "Epoch: 66 | Loss: 0.7635058164596558 \n",
            "Epoch: 67/100 | Loss: 0.7635\n",
            "Epoch: 67 | Loss: 0.7622183561325073 \n",
            "Epoch: 68/100 | Loss: 0.7622\n",
            "Epoch: 68 | Loss: 0.7609446048736572 \n",
            "Epoch: 69/100 | Loss: 0.7609\n",
            "Epoch: 69 | Loss: 0.7596847414970398 \n",
            "Epoch: 70/100 | Loss: 0.7597\n",
            "Epoch: 70 | Loss: 0.7584385275840759 \n",
            "Epoch: 71/100 | Loss: 0.7584\n",
            "Epoch: 71 | Loss: 0.7572055459022522 \n",
            "Epoch: 72/100 | Loss: 0.7572\n",
            "Epoch: 72 | Loss: 0.7559862732887268 \n",
            "Epoch: 73/100 | Loss: 0.7560\n",
            "Epoch: 73 | Loss: 0.7547798752784729 \n",
            "Epoch: 74/100 | Loss: 0.7548\n",
            "Epoch: 74 | Loss: 0.7535867094993591 \n",
            "Epoch: 75/100 | Loss: 0.7536\n",
            "Epoch: 75 | Loss: 0.7524061799049377 \n",
            "Epoch: 76/100 | Loss: 0.7524\n",
            "Epoch: 76 | Loss: 0.7512386441230774 \n",
            "Epoch: 77/100 | Loss: 0.7512\n",
            "Epoch: 77 | Loss: 0.7500838041305542 \n",
            "Epoch: 78/100 | Loss: 0.7501\n",
            "Epoch: 78 | Loss: 0.7489414215087891 \n",
            "Epoch: 79/100 | Loss: 0.7489\n",
            "Epoch: 79 | Loss: 0.747811496257782 \n",
            "Epoch: 80/100 | Loss: 0.7478\n",
            "Epoch: 80 | Loss: 0.7466937899589539 \n",
            "Epoch: 81/100 | Loss: 0.7467\n",
            "Epoch: 81 | Loss: 0.7455882430076599 \n",
            "Epoch: 82/100 | Loss: 0.7456\n",
            "Epoch: 82 | Loss: 0.7444947957992554 \n",
            "Epoch: 83/100 | Loss: 0.7445\n",
            "Epoch: 83 | Loss: 0.743412971496582 \n",
            "Epoch: 84/100 | Loss: 0.7434\n",
            "Epoch: 84 | Loss: 0.7423431873321533 \n",
            "Epoch: 85/100 | Loss: 0.7423\n",
            "Epoch: 85 | Loss: 0.7412846684455872 \n",
            "Epoch: 86/100 | Loss: 0.7413\n",
            "Epoch: 86 | Loss: 0.7402380704879761 \n",
            "Epoch: 87/100 | Loss: 0.7402\n",
            "Epoch: 87 | Loss: 0.7392025589942932 \n",
            "Epoch: 88/100 | Loss: 0.7392\n",
            "Epoch: 88 | Loss: 0.7381784915924072 \n",
            "Epoch: 89/100 | Loss: 0.7382\n",
            "Epoch: 89 | Loss: 0.7371655106544495 \n",
            "Epoch: 90/100 | Loss: 0.7372\n",
            "Epoch: 90 | Loss: 0.7361636757850647 \n",
            "Epoch: 91/100 | Loss: 0.7362\n",
            "Epoch: 91 | Loss: 0.7351727485656738 \n",
            "Epoch: 92/100 | Loss: 0.7352\n",
            "Epoch: 92 | Loss: 0.7341923117637634 \n",
            "Epoch: 93/100 | Loss: 0.7342\n",
            "Epoch: 93 | Loss: 0.7332231402397156 \n",
            "Epoch: 94/100 | Loss: 0.7332\n",
            "Epoch: 94 | Loss: 0.7322638034820557 \n",
            "Epoch: 95/100 | Loss: 0.7323\n",
            "Epoch: 95 | Loss: 0.7313156723976135 \n",
            "Epoch: 96/100 | Loss: 0.7313\n",
            "Epoch: 96 | Loss: 0.7303773760795593 \n",
            "Epoch: 97/100 | Loss: 0.7304\n",
            "Epoch: 97 | Loss: 0.7294495105743408 \n",
            "Epoch: 98/100 | Loss: 0.7294\n",
            "Epoch: 98 | Loss: 0.728531539440155 \n",
            "Epoch: 99/100 | Loss: 0.7285\n",
            "Epoch: 99 | Loss: 0.7276238203048706 \n",
            "Epoch: 100/100 | Loss: 0.7276\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}