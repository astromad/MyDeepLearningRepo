{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Normalize.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMuTt+LK7bhCNMi7v1BQ1Od",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astromad/MyDeepLearningRepo/blob/master/Normalize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42LNkB9ZVdFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC7ALMjCYjja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "74488b2b-7fad-4993-e2d1-098ed4629711"
      },
      "source": [
        "!pip install faker\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting faker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/21/a0c2ed68ca140d0a5bd0ac762f05cbe03dd52ffb577ae5d5c973dc85e103/Faker-4.1.1-py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.6/dist-packages (from faker) (2.8.1)\n",
            "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from faker) (1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.4->faker) (1.12.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw9LxDZiY-HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from faker import Faker"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdui2mrPV2xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fake = Faker()\n",
        "Faker.seed(12345)\n",
        "random.seed(12345)\n",
        "\n",
        "# Define format of the data we would like to generate\n",
        "FORMATS = ['short',\n",
        "           'medium',\n",
        "           'long',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'd MMM YYY', \n",
        "           'd MMMM YYY',\n",
        "           'dd MMM YYY',\n",
        "           'd MMM, YYY',\n",
        "           'd MMMM, YYY',\n",
        "           'dd, MMM YYY',\n",
        "           'd MM YY',\n",
        "           'd MMMM YYY',\n",
        "           'MMMM d YYY',\n",
        "           'MMMM d, YYY',\n",
        "           'dd.MM.YY']\n",
        "\n",
        "# change this if you want it to work with another language\n",
        "LOCALES = ['en_US']\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX401RRXZMNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_date():\n",
        "    \"\"\"\n",
        "        Loads some fake dates \n",
        "        :returns: tuple containing human readable string, machine readable string, and date object\n",
        "    \"\"\"\n",
        "    dt = fake.date_object()\n",
        "\n",
        "    try:\n",
        "        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US') # locale=random.choice(LOCALES))\n",
        "        human_readable = human_readable.lower()\n",
        "        human_readable = human_readable.replace(',','')\n",
        "        machine_readable = dt.isoformat()\n",
        "        \n",
        "    except AttributeError as e:\n",
        "        return None, None, None\n",
        "\n",
        "    return human_readable, machine_readable, dt\n",
        "\n",
        "def load_dataset(m):\n",
        "    \"\"\"\n",
        "        Loads a dataset with m examples and vocabularies\n",
        "        :m: the number of examples to generate\n",
        "    \"\"\"\n",
        "    \n",
        "    human_vocab = set()\n",
        "    machine_vocab = set()\n",
        "    dataset = []\n",
        "    Tx = 30\n",
        "    \n",
        "\n",
        "    for i in tqdm(range(m)):\n",
        "        h, m, _ = load_date()\n",
        "        if h is not None:\n",
        "            dataset.append((h, m))\n",
        "            human_vocab.update(tuple(h))\n",
        "            machine_vocab.update(tuple(m))\n",
        "    \n",
        "    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'], \n",
        "                     list(range(len(human_vocab) + 2))))\n",
        "    inv_machine = dict(enumerate(sorted(machine_vocab)))\n",
        "    machine = {v:k for k,v in inv_machine.items()}\n",
        " \n",
        "    return dataset, human, machine, inv_machine"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om_Z0zb6VwTg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec77639a-bb16-44bc-b0db-d802e9a2149c"
      },
      "source": [
        "m = 10000\n",
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 20944.82it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmTphf4rZbfp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d364d9f6-9f82-4771-e153-66e8826f843f"
      },
      "source": [
        "dataset[:10]\n",
        "print(dataset[:10])\n",
        "print(machine_vocab)\n",
        "print(inv_machine_vocab)\n",
        "print(human_vocab)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('9 may 1998', '1998-05-09'), ('10.11.19', '2019-11-10'), ('9/10/70', '1970-09-10'), ('saturday april 28 1990', '1990-04-28'), ('thursday january 26 1995', '1995-01-26'), ('monday march 7 1983', '1983-03-07'), ('sunday may 22 1988', '1988-05-22'), ('08 jul 2008', '2008-07-08'), ('8 sep 1999', '1999-09-08'), ('thursday january 1 1981', '1981-01-01')]\n",
            "{'-': 0, '0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10}\n",
            "{0: '-', 1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9'}\n",
            "{' ': 0, '.': 1, '/': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, 'a': 13, 'b': 14, 'c': 15, 'd': 16, 'e': 17, 'f': 18, 'g': 19, 'h': 20, 'i': 21, 'j': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'y': 34, '<unk>': 35, '<pad>': 36}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9Bhe4lFZiNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
        "    \n",
        "    X, Y = zip(*dataset)\n",
        "    \n",
        "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
        "    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n",
        "    \n",
        "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
        "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
        "\n",
        "    return X, np.array(Y), Xoh, Yoh\n",
        "\n",
        "def string_to_int(string, length, vocab):\n",
        "    \"\"\"\n",
        "    Converts all strings in the vocabulary into a list of integers representing the positions of the\n",
        "    input string's characters in the \"vocab\"\n",
        "    \n",
        "    Arguments:\n",
        "    string -- input string, e.g. 'Wed 10 Jul 2007'\n",
        "    length -- the number of time steps you'd like, determines if the output will be padded or cut\n",
        "    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n",
        "    \n",
        "    Returns:\n",
        "    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n",
        "    \"\"\"\n",
        "    \n",
        "    #make lower to standardize\n",
        "    string = string.lower()\n",
        "    string = string.replace(',','')\n",
        "    \n",
        "    if len(string) > length:\n",
        "        string = string[:length]\n",
        "        \n",
        "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
        "    \n",
        "    if len(string) < length:\n",
        "        rep += [vocab['<pad>']] * (length - len(string))\n",
        "    \n",
        "    #print (rep)\n",
        "    return rep\n",
        "\n",
        "\n",
        "def int_to_string(ints, inv_vocab):\n",
        "    \"\"\"\n",
        "    Output a machine readable list of characters based on a list of indexes in the machine's vocabulary\n",
        "    \n",
        "    Arguments:\n",
        "    ints -- list of integers representing indexes in the machine's vocabulary\n",
        "    inv_vocab -- dictionary mapping machine readable indexes to machine readable characters \n",
        "    \n",
        "    Returns:\n",
        "    l -- list of characters corresponding to the indexes of ints thanks to the inv_vocab mapping\n",
        "    \"\"\"\n",
        "    \n",
        "    l = [inv_vocab[i] for i in ints]\n",
        "    return l\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fENc--7YZtaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b6c004f8-595e-476b-b0b0-8c8e3c91dd82"
      },
      "source": [
        "Tx = 30\n",
        "Ty = 10\n",
        "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
        "\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(X[0,:])\n",
        "print(\"Y.shape:\", Y.shape)\n",
        "print(\"Xoh.shape:\", Xoh.shape)\n",
        "print(Xoh[0,0,:])\n",
        "print(\"Yoh.shape:\", Yoh.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape: (10000, 30)\n",
            "[12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
            " 36 36 36 36 36 36]\n",
            "Y.shape: (10000, 10)\n",
            "Xoh.shape: (10000, 30, 37)\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Yoh.shape: (10000, 10, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S_rsBa_ZzEn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "02876d84-c489-450e-a068-62cdeb63a386"
      },
      "source": [
        "index = 0\n",
        "print(\"Source date:\", dataset[index][0])\n",
        "print(\"Target date:\", dataset[index][1])\n",
        "print()\n",
        "print(\"Source after preprocessing (indices):\", X[index])\n",
        "print(\"Target after preprocessing (indices):\", Y[index])\n",
        "print()\n",
        "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
        "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source date: 9 may 1998\n",
            "Target date: 1998-05-09\n",
            "\n",
            "Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
            " 36 36 36 36 36 36]\n",
            "Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n",
            "\n",
            "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7tIc_rgaCWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x, axis=1):\n",
        "    \"\"\"Softmax activation function.\n",
        "    # Arguments\n",
        "        x : Tensor.\n",
        "        axis: Integer, axis along which the softmax normalization is applied.\n",
        "    # Returns\n",
        "        Tensor, output of softmax transformation.\n",
        "    # Raises\n",
        "        ValueError: In case `dim(x) == 1`.\n",
        "    \"\"\"\n",
        "    ndim = K.ndim(x)\n",
        "    if ndim == 2:\n",
        "        return K.softmax(x)\n",
        "    elif ndim > 2:\n",
        "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "        s = K.sum(e, axis=axis, keepdims=True)\n",
        "        return e / s\n",
        "    else:\n",
        "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
        "        \n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBNQ8kDKZ5ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defined shared layers as global variables\n",
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor1 = Dense(10, activation = \"tanh\")\n",
        "densor2 = Dense(1, activation = \"relu\")\n",
        "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
        "dotor = Dot(axes = 1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb7l5we0aL_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_step_attention(a, s_prev):\n",
        "    \"\"\"\n",
        "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
        "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
        "    \n",
        "    Arguments:\n",
        "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
        "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
        "    \n",
        "    Returns:\n",
        "    context -- context vector, input of the next (post-attention) LSTM cell\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
        "    s_prev = repeator(s_prev)\n",
        "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
        "    # For grading purposes, please list 'a' first and 's_prev' second, in this order.\n",
        "    concat = concatenator([a,s_prev])\n",
        "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
        "    e = densor1(concat)\n",
        "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
        "    energies = densor2(e)\n",
        "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
        "    alphas = activator(energies)\n",
        "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
        "    context = dotor([alphas,a])\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return context"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQTmIhG1aRW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_a = 32 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
        "n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n",
        "\n",
        "# Please note, this is the post attention LSTM cell.  \n",
        "# For the purposes of passing the automatic grader\n",
        "# please do not modify this global variable.  This will be corrected once the automatic grader is also updated.\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # post-attention LSTM \n",
        "output_layer = Dense(len(machine_vocab), activation=softmax)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec_m4TeAaWgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    Tx -- length of the input sequence\n",
        "    Ty -- length of the output sequence\n",
        "    n_a -- hidden state size of the Bi-LSTM\n",
        "    n_s -- hidden state size of the post-attention LSTM\n",
        "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
        "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
        "\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the inputs of your model with a shape (Tx,)\n",
        "    # Define s0 (initial hidden state) and c0 (initial cell state)\n",
        "    # for the decoder LSTM with shape (n_s,)\n",
        "    X = Input(shape=(Tx, human_vocab_size))\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    \n",
        "    # Initialize empty list of outputs\n",
        "    outputs = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Step 1: Define your pre-attention Bi-LSTM. (≈ 1 line)\n",
        "    a = Bidirectional(LSTM(units=n_a, return_sequences=True))(X)\n",
        "    \n",
        "    # Step 2: Iterate for Ty steps\n",
        "    for t in range(Ty):\n",
        "    \n",
        "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
        "        context = one_step_attention(a, s)\n",
        "        \n",
        "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
        "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
        "        s, _, c = post_activation_LSTM_cell(inputs=context, initial_state=[s, c])\n",
        "        \n",
        "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
        "        out = output_layer(inputs=s)\n",
        "        \n",
        "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
        "        outputs.append(out)\n",
        "    \n",
        "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
        "    model = Model(inputs=[X,s0,c0], outputs=outputs)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqTuZzmLabwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubpGZz1Tagvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85fca664-f15c-479b-a53f-b06bbc2ee426"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_2 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_2[0][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_2[1][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_2[2][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_2[3][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_2[4][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_2[5][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_2[6][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_2[7][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_2[8][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_2[9][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 30, 10)       1290        concatenate_2[0][0]              \n",
            "                                                                 concatenate_2[1][0]              \n",
            "                                                                 concatenate_2[2][0]              \n",
            "                                                                 concatenate_2[3][0]              \n",
            "                                                                 concatenate_2[4][0]              \n",
            "                                                                 concatenate_2[5][0]              \n",
            "                                                                 concatenate_2[6][0]              \n",
            "                                                                 concatenate_2[7][0]              \n",
            "                                                                 concatenate_2[8][0]              \n",
            "                                                                 concatenate_2[9][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 30, 1)        11          dense_3[0][0]                    \n",
            "                                                                 dense_3[1][0]                    \n",
            "                                                                 dense_3[2][0]                    \n",
            "                                                                 dense_3[3][0]                    \n",
            "                                                                 dense_3[4][0]                    \n",
            "                                                                 dense_3[5][0]                    \n",
            "                                                                 dense_3[6][0]                    \n",
            "                                                                 dense_3[7][0]                    \n",
            "                                                                 dense_3[8][0]                    \n",
            "                                                                 dense_3[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights (Activation)  (None, 30, 1)        0           dense_4[0][0]                    \n",
            "                                                                 dense_4[1][0]                    \n",
            "                                                                 dense_4[2][0]                    \n",
            "                                                                 dense_4[3][0]                    \n",
            "                                                                 dense_4[4][0]                    \n",
            "                                                                 dense_4[5][0]                    \n",
            "                                                                 dense_4[6][0]                    \n",
            "                                                                 dense_4[7][0]                    \n",
            "                                                                 dense_4[8][0]                    \n",
            "                                                                 dense_4[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[1][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[2][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[3][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[4][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[5][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[6][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[7][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[8][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[9][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 dot_1[1][0]                      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "                                                                 dot_1[2][0]                      \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[1][2]                     \n",
            "                                                                 dot_1[3][0]                      \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[2][2]                     \n",
            "                                                                 dot_1[4][0]                      \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[3][2]                     \n",
            "                                                                 dot_1[5][0]                      \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[4][2]                     \n",
            "                                                                 dot_1[6][0]                      \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[5][2]                     \n",
            "                                                                 dot_1[7][0]                      \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[6][2]                     \n",
            "                                                                 dot_1[8][0]                      \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[7][2]                     \n",
            "                                                                 dot_1[9][0]                      \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[8][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[9][0]                     \n",
            "==================================================================================================\n",
            "Total params: 52,960\n",
            "Trainable params: 52,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05p7Trx9alWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRJz-VyKatG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "outputs = list(Yoh.swapaxes(0,1))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7kGs-0waz8G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "07e38bf0-7883-4d57-8268-ee4b44e5c2a3"
      },
      "source": [
        "model.fit([Xoh, s0, c0], outputs, epochs=3, batch_size=100)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 9s 947us/step - loss: 8.0682 - dense_5_loss: 1.7484 - dense_5_accuracy: 0.9579 - dense_5_accuracy_1: 0.9642 - dense_5_accuracy_2: 0.6496 - dense_5_accuracy_3: 0.2804 - dense_5_accuracy_4: 1.0000 - dense_5_accuracy_5: 0.8792 - dense_5_accuracy_6: 0.3638 - dense_5_accuracy_7: 0.9998 - dense_5_accuracy_8: 0.6415 - dense_5_accuracy_9: 0.3716\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 9s 927us/step - loss: 5.5902 - dense_5_loss: 1.0950 - dense_5_accuracy: 0.9723 - dense_5_accuracy_1: 0.9759 - dense_5_accuracy_2: 0.7933 - dense_5_accuracy_3: 0.5612 - dense_5_accuracy_4: 0.9999 - dense_5_accuracy_5: 0.9265 - dense_5_accuracy_6: 0.4814 - dense_5_accuracy_7: 1.0000 - dense_5_accuracy_8: 0.7506 - dense_5_accuracy_9: 0.6383\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 9s 926us/step - loss: 3.6003 - dense_5_loss: 0.5955 - dense_5_accuracy: 0.9777 - dense_5_accuracy_1: 0.9834 - dense_5_accuracy_2: 0.8884 - dense_5_accuracy_3: 0.8472 - dense_5_accuracy_4: 1.0000 - dense_5_accuracy_5: 0.9557 - dense_5_accuracy_6: 0.6253 - dense_5_accuracy_7: 1.0000 - dense_5_accuracy_8: 0.8099 - dense_5_accuracy_9: 0.8253\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1568073eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1hZb99ebG7b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "87a102b9-1b2a-455e-fd36-a4eef05858c6"
      },
      "source": [
        "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
        "for example in EXAMPLES:\n",
        "    \n",
        "    source = string_to_int(example, Tx, human_vocab)\n",
        "    #print(source)\n",
        "    #source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
        "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)), ndmin=3)\n",
        "    #source = np.array([string_to_int(i, Tx, human_vocab) for i in EXAMPLES])\n",
        "    #source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
        "\n",
        "    prediction = model.predict([source, s0, c0])\n",
        "    prediction = np.argmax(prediction, axis = -1)\n",
        "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
        "    \n",
        "    print(\"source:\", example)\n",
        "    print(\"output:\", ''.join(output),\"\\n\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source: 3 May 1979\n",
            "output: 1979-05-03 \n",
            "\n",
            "source: 5 April 09\n",
            "output: 2009-04-04 \n",
            "\n",
            "source: 21th of August 2016\n",
            "output: 2016-06-26 \n",
            "\n",
            "source: Tue 10 Jul 2007\n",
            "output: 2007-07-10 \n",
            "\n",
            "source: Saturday May 9 2018\n",
            "output: 2018-05-09 \n",
            "\n",
            "source: March 3 2001\n",
            "output: 2001-03-03 \n",
            "\n",
            "source: March 3rd 2001\n",
            "output: 2001-03-03 \n",
            "\n",
            "source: 1 March 2001\n",
            "output: 2000-03-11 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}