{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyTorch_LinearRegression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOuZvgwBl62eCKicwZNURpS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astromad/MyDeepLearningRepo/blob/master/pyTorch_LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRyOkErZtztu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "53ad44f5-9269-4354-a4c4-09e1ab1aae1a"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(500):\n",
        "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 2) Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# After training\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 43.74241256713867 \n",
            "Epoch: 1 | Loss: 20.047639846801758 \n",
            "Epoch: 2 | Loss: 9.491141319274902 \n",
            "Epoch: 3 | Loss: 4.783547401428223 \n",
            "Epoch: 4 | Loss: 2.67983341217041 \n",
            "Epoch: 5 | Loss: 1.7354135513305664 \n",
            "Epoch: 6 | Loss: 1.3071881532669067 \n",
            "Epoch: 7 | Loss: 1.1088707447052002 \n",
            "Epoch: 8 | Loss: 1.0130119323730469 \n",
            "Epoch: 9 | Loss: 0.9628742933273315 \n",
            "Epoch: 10 | Loss: 0.9331979155540466 \n",
            "Epoch: 11 | Loss: 0.9127354621887207 \n",
            "Epoch: 12 | Loss: 0.8964787125587463 \n",
            "Epoch: 13 | Loss: 0.8821978569030762 \n",
            "Epoch: 14 | Loss: 0.8688967227935791 \n",
            "Epoch: 15 | Loss: 0.8561328649520874 \n",
            "Epoch: 16 | Loss: 0.8437055349349976 \n",
            "Epoch: 17 | Loss: 0.8315252065658569 \n",
            "Epoch: 18 | Loss: 0.8195503354072571 \n",
            "Epoch: 19 | Loss: 0.8077616095542908 \n",
            "Epoch: 20 | Loss: 0.7961475253105164 \n",
            "Epoch: 21 | Loss: 0.7847037315368652 \n",
            "Epoch: 22 | Loss: 0.773425281047821 \n",
            "Epoch: 23 | Loss: 0.7623096108436584 \n",
            "Epoch: 24 | Loss: 0.7513537406921387 \n",
            "Epoch: 25 | Loss: 0.7405555248260498 \n",
            "Epoch: 26 | Loss: 0.7299126386642456 \n",
            "Epoch: 27 | Loss: 0.7194227576255798 \n",
            "Epoch: 28 | Loss: 0.7090833187103271 \n",
            "Epoch: 29 | Loss: 0.6988928318023682 \n",
            "Epoch: 30 | Loss: 0.6888481378555298 \n",
            "Epoch: 31 | Loss: 0.6789484620094299 \n",
            "Epoch: 32 | Loss: 0.6691913604736328 \n",
            "Epoch: 33 | Loss: 0.6595736145973206 \n",
            "Epoch: 34 | Loss: 0.6500943899154663 \n",
            "Epoch: 35 | Loss: 0.6407514810562134 \n",
            "Epoch: 36 | Loss: 0.6315431594848633 \n",
            "Epoch: 37 | Loss: 0.6224669218063354 \n",
            "Epoch: 38 | Loss: 0.6135213971138 \n",
            "Epoch: 39 | Loss: 0.6047040224075317 \n",
            "Epoch: 40 | Loss: 0.5960131287574768 \n",
            "Epoch: 41 | Loss: 0.5874478816986084 \n",
            "Epoch: 42 | Loss: 0.5790053009986877 \n",
            "Epoch: 43 | Loss: 0.570684015750885 \n",
            "Epoch: 44 | Loss: 0.5624822378158569 \n",
            "Epoch: 45 | Loss: 0.554398238658905 \n",
            "Epoch: 46 | Loss: 0.5464310646057129 \n",
            "Epoch: 47 | Loss: 0.5385776162147522 \n",
            "Epoch: 48 | Loss: 0.5308376550674438 \n",
            "Epoch: 49 | Loss: 0.523208737373352 \n",
            "Epoch: 50 | Loss: 0.5156896710395813 \n",
            "Epoch: 51 | Loss: 0.5082782506942749 \n",
            "Epoch: 52 | Loss: 0.5009732842445374 \n",
            "Epoch: 53 | Loss: 0.493773877620697 \n",
            "Epoch: 54 | Loss: 0.48667746782302856 \n",
            "Epoch: 55 | Loss: 0.47968319058418274 \n",
            "Epoch: 56 | Loss: 0.4727895259857178 \n",
            "Epoch: 57 | Loss: 0.4659942090511322 \n",
            "Epoch: 58 | Loss: 0.4592975080013275 \n",
            "Epoch: 59 | Loss: 0.4526965022087097 \n",
            "Epoch: 60 | Loss: 0.4461905062198639 \n",
            "Epoch: 61 | Loss: 0.43977832794189453 \n",
            "Epoch: 62 | Loss: 0.4334579110145569 \n",
            "Epoch: 63 | Loss: 0.42722848057746887 \n",
            "Epoch: 64 | Loss: 0.42108848690986633 \n",
            "Epoch: 65 | Loss: 0.4150369167327881 \n",
            "Epoch: 66 | Loss: 0.40907180309295654 \n",
            "Epoch: 67 | Loss: 0.40319305658340454 \n",
            "Epoch: 68 | Loss: 0.39739876985549927 \n",
            "Epoch: 69 | Loss: 0.3916873633861542 \n",
            "Epoch: 70 | Loss: 0.38605841994285583 \n",
            "Epoch: 71 | Loss: 0.3805098235607147 \n",
            "Epoch: 72 | Loss: 0.37504124641418457 \n",
            "Epoch: 73 | Loss: 0.3696514368057251 \n",
            "Epoch: 74 | Loss: 0.3643391728401184 \n",
            "Epoch: 75 | Loss: 0.35910290479660034 \n",
            "Epoch: 76 | Loss: 0.35394203662872314 \n",
            "Epoch: 77 | Loss: 0.3488553762435913 \n",
            "Epoch: 78 | Loss: 0.34384170174598694 \n",
            "Epoch: 79 | Loss: 0.33889999985694885 \n",
            "Epoch: 80 | Loss: 0.33402949571609497 \n",
            "Epoch: 81 | Loss: 0.3292289972305298 \n",
            "Epoch: 82 | Loss: 0.32449766993522644 \n",
            "Epoch: 83 | Loss: 0.319833904504776 \n",
            "Epoch: 84 | Loss: 0.3152375817298889 \n",
            "Epoch: 85 | Loss: 0.310707151889801 \n",
            "Epoch: 86 | Loss: 0.30624157190322876 \n",
            "Epoch: 87 | Loss: 0.3018406629562378 \n",
            "Epoch: 88 | Loss: 0.29750242829322815 \n",
            "Epoch: 89 | Loss: 0.2932269871234894 \n",
            "Epoch: 90 | Loss: 0.289012610912323 \n",
            "Epoch: 91 | Loss: 0.2848590612411499 \n",
            "Epoch: 92 | Loss: 0.28076550364494324 \n",
            "Epoch: 93 | Loss: 0.27673032879829407 \n",
            "Epoch: 94 | Loss: 0.2727532982826233 \n",
            "Epoch: 95 | Loss: 0.2688335180282593 \n",
            "Epoch: 96 | Loss: 0.26496997475624084 \n",
            "Epoch: 97 | Loss: 0.2611621618270874 \n",
            "Epoch: 98 | Loss: 0.25740841031074524 \n",
            "Epoch: 99 | Loss: 0.2537093758583069 \n",
            "Epoch: 100 | Loss: 0.25006309151649475 \n",
            "Epoch: 101 | Loss: 0.24646928906440735 \n",
            "Epoch: 102 | Loss: 0.24292683601379395 \n",
            "Epoch: 103 | Loss: 0.23943568766117096 \n",
            "Epoch: 104 | Loss: 0.23599474132061005 \n",
            "Epoch: 105 | Loss: 0.23260292410850525 \n",
            "Epoch: 106 | Loss: 0.22926032543182373 \n",
            "Epoch: 107 | Loss: 0.225965678691864 \n",
            "Epoch: 108 | Loss: 0.22271789610385895 \n",
            "Epoch: 109 | Loss: 0.21951711177825928 \n",
            "Epoch: 110 | Loss: 0.2163623720407486 \n",
            "Epoch: 111 | Loss: 0.2132529318332672 \n",
            "Epoch: 112 | Loss: 0.2101878672838211 \n",
            "Epoch: 113 | Loss: 0.20716732740402222 \n",
            "Epoch: 114 | Loss: 0.2041899561882019 \n",
            "Epoch: 115 | Loss: 0.20125563442707062 \n",
            "Epoch: 116 | Loss: 0.19836312532424927 \n",
            "Epoch: 117 | Loss: 0.19551250338554382 \n",
            "Epoch: 118 | Loss: 0.1927025467157364 \n",
            "Epoch: 119 | Loss: 0.189933180809021 \n",
            "Epoch: 120 | Loss: 0.18720361590385437 \n",
            "Epoch: 121 | Loss: 0.18451295793056488 \n",
            "Epoch: 122 | Loss: 0.18186131119728088 \n",
            "Epoch: 123 | Loss: 0.17924775183200836 \n",
            "Epoch: 124 | Loss: 0.17667177319526672 \n",
            "Epoch: 125 | Loss: 0.17413271963596344 \n",
            "Epoch: 126 | Loss: 0.17163008451461792 \n",
            "Epoch: 127 | Loss: 0.16916339099407196 \n",
            "Epoch: 128 | Loss: 0.16673246026039124 \n",
            "Epoch: 129 | Loss: 0.16433602571487427 \n",
            "Epoch: 130 | Loss: 0.16197429597377777 \n",
            "Epoch: 131 | Loss: 0.15964661538600922 \n",
            "Epoch: 132 | Loss: 0.1573520302772522 \n",
            "Epoch: 133 | Loss: 0.15509065985679626 \n",
            "Epoch: 134 | Loss: 0.1528617888689041 \n",
            "Epoch: 135 | Loss: 0.15066486597061157 \n",
            "Epoch: 136 | Loss: 0.14849981665611267 \n",
            "Epoch: 137 | Loss: 0.14636550843715668 \n",
            "Epoch: 138 | Loss: 0.14426201581954956 \n",
            "Epoch: 139 | Loss: 0.14218869805335999 \n",
            "Epoch: 140 | Loss: 0.14014528691768646 \n",
            "Epoch: 141 | Loss: 0.13813118636608124 \n",
            "Epoch: 142 | Loss: 0.1361459642648697 \n",
            "Epoch: 143 | Loss: 0.1341894567012787 \n",
            "Epoch: 144 | Loss: 0.13226094841957092 \n",
            "Epoch: 145 | Loss: 0.13036009669303894 \n",
            "Epoch: 146 | Loss: 0.12848660349845886 \n",
            "Epoch: 147 | Loss: 0.12664024531841278 \n",
            "Epoch: 148 | Loss: 0.1248200461268425 \n",
            "Epoch: 149 | Loss: 0.12302622944116592 \n",
            "Epoch: 150 | Loss: 0.12125805765390396 \n",
            "Epoch: 151 | Loss: 0.1195153146982193 \n",
            "Epoch: 152 | Loss: 0.11779771745204926 \n",
            "Epoch: 153 | Loss: 0.116104856133461 \n",
            "Epoch: 154 | Loss: 0.11443627625703812 \n",
            "Epoch: 155 | Loss: 0.11279163509607315 \n",
            "Epoch: 156 | Loss: 0.11117072403430939 \n",
            "Epoch: 157 | Loss: 0.10957291722297668 \n",
            "Epoch: 158 | Loss: 0.10799826681613922 \n",
            "Epoch: 159 | Loss: 0.10644619166851044 \n",
            "Epoch: 160 | Loss: 0.10491630434989929 \n",
            "Epoch: 161 | Loss: 0.10340859740972519 \n",
            "Epoch: 162 | Loss: 0.10192229598760605 \n",
            "Epoch: 163 | Loss: 0.10045763105154037 \n",
            "Epoch: 164 | Loss: 0.09901383519172668 \n",
            "Epoch: 165 | Loss: 0.09759092330932617 \n",
            "Epoch: 166 | Loss: 0.09618827700614929 \n",
            "Epoch: 167 | Loss: 0.09480597078800201 \n",
            "Epoch: 168 | Loss: 0.09344348311424255 \n",
            "Epoch: 169 | Loss: 0.09210048615932465 \n",
            "Epoch: 170 | Loss: 0.09077703952789307 \n",
            "Epoch: 171 | Loss: 0.08947228640317917 \n",
            "Epoch: 172 | Loss: 0.08818649500608444 \n",
            "Epoch: 173 | Loss: 0.08691896498203278 \n",
            "Epoch: 174 | Loss: 0.08566974848508835 \n",
            "Epoch: 175 | Loss: 0.084438756108284 \n",
            "Epoch: 176 | Loss: 0.08322510123252869 \n",
            "Epoch: 177 | Loss: 0.08202911913394928 \n",
            "Epoch: 178 | Loss: 0.08085019886493683 \n",
            "Epoch: 179 | Loss: 0.07968822121620178 \n",
            "Epoch: 180 | Loss: 0.07854297012090683 \n",
            "Epoch: 181 | Loss: 0.0774141326546669 \n",
            "Epoch: 182 | Loss: 0.07630172371864319 \n",
            "Epoch: 183 | Loss: 0.07520495355129242 \n",
            "Epoch: 184 | Loss: 0.07412420213222504 \n",
            "Epoch: 185 | Loss: 0.07305880635976791 \n",
            "Epoch: 186 | Loss: 0.07200904190540314 \n",
            "Epoch: 187 | Loss: 0.07097400724887848 \n",
            "Epoch: 188 | Loss: 0.06995409727096558 \n",
            "Epoch: 189 | Loss: 0.06894874572753906 \n",
            "Epoch: 190 | Loss: 0.06795778125524521 \n",
            "Epoch: 191 | Loss: 0.06698111444711685 \n",
            "Epoch: 192 | Loss: 0.0660184994339943 \n",
            "Epoch: 193 | Loss: 0.06506974995136261 \n",
            "Epoch: 194 | Loss: 0.06413474678993225 \n",
            "Epoch: 195 | Loss: 0.06321287155151367 \n",
            "Epoch: 196 | Loss: 0.06230441480875015 \n",
            "Epoch: 197 | Loss: 0.06140902638435364 \n",
            "Epoch: 198 | Loss: 0.06052638962864876 \n",
            "Epoch: 199 | Loss: 0.059656623750925064 \n",
            "Epoch: 200 | Loss: 0.058799292892217636 \n",
            "Epoch: 201 | Loss: 0.057954199612140656 \n",
            "Epoch: 202 | Loss: 0.057121362537145615 \n",
            "Epoch: 203 | Loss: 0.056300289928913116 \n",
            "Epoch: 204 | Loss: 0.055491264909505844 \n",
            "Epoch: 205 | Loss: 0.054693639278411865 \n",
            "Epoch: 206 | Loss: 0.05390775203704834 \n",
            "Epoch: 207 | Loss: 0.0531330332159996 \n",
            "Epoch: 208 | Loss: 0.05236944556236267 \n",
            "Epoch: 209 | Loss: 0.0516168549656868 \n",
            "Epoch: 210 | Loss: 0.05087494105100632 \n",
            "Epoch: 211 | Loss: 0.05014374107122421 \n",
            "Epoch: 212 | Loss: 0.04942316934466362 \n",
            "Epoch: 213 | Loss: 0.0487128421664238 \n",
            "Epoch: 214 | Loss: 0.04801275581121445 \n",
            "Epoch: 215 | Loss: 0.04732281714677811 \n",
            "Epoch: 216 | Loss: 0.046642642468214035 \n",
            "Epoch: 217 | Loss: 0.04597226902842522 \n",
            "Epoch: 218 | Loss: 0.04531159996986389 \n",
            "Epoch: 219 | Loss: 0.044660333544015884 \n",
            "Epoch: 220 | Loss: 0.04401853680610657 \n",
            "Epoch: 221 | Loss: 0.043385881930589676 \n",
            "Epoch: 222 | Loss: 0.04276240989565849 \n",
            "Epoch: 223 | Loss: 0.04214790090918541 \n",
            "Epoch: 224 | Loss: 0.0415421761572361 \n",
            "Epoch: 225 | Loss: 0.0409451425075531 \n",
            "Epoch: 226 | Loss: 0.040356650948524475 \n",
            "Epoch: 227 | Loss: 0.039776623249053955 \n",
            "Epoch: 228 | Loss: 0.039204929023981094 \n",
            "Epoch: 229 | Loss: 0.038641590625047684 \n",
            "Epoch: 230 | Loss: 0.038086291402578354 \n",
            "Epoch: 231 | Loss: 0.03753887116909027 \n",
            "Epoch: 232 | Loss: 0.036999430507421494 \n",
            "Epoch: 233 | Loss: 0.03646763786673546 \n",
            "Epoch: 234 | Loss: 0.03594348207116127 \n",
            "Epoch: 235 | Loss: 0.03542697802186012 \n",
            "Epoch: 236 | Loss: 0.034917838871479034 \n",
            "Epoch: 237 | Loss: 0.034416016191244125 \n",
            "Epoch: 238 | Loss: 0.03392135351896286 \n",
            "Epoch: 239 | Loss: 0.0334339402616024 \n",
            "Epoch: 240 | Loss: 0.0329534187912941 \n",
            "Epoch: 241 | Loss: 0.03247975930571556 \n",
            "Epoch: 242 | Loss: 0.03201303631067276 \n",
            "Epoch: 243 | Loss: 0.031552914530038834 \n",
            "Epoch: 244 | Loss: 0.0310994740575552 \n",
            "Epoch: 245 | Loss: 0.030652552843093872 \n",
            "Epoch: 246 | Loss: 0.030212029814720154 \n",
            "Epoch: 247 | Loss: 0.029777826741337776 \n",
            "Epoch: 248 | Loss: 0.02934982255101204 \n",
            "Epoch: 249 | Loss: 0.02892809361219406 \n",
            "Epoch: 250 | Loss: 0.028512360528111458 \n",
            "Epoch: 251 | Loss: 0.02810261957347393 \n",
            "Epoch: 252 | Loss: 0.027698669582605362 \n",
            "Epoch: 253 | Loss: 0.027300594374537468 \n",
            "Epoch: 254 | Loss: 0.02690822258591652 \n",
            "Epoch: 255 | Loss: 0.026521530002355576 \n",
            "Epoch: 256 | Loss: 0.02614039182662964 \n",
            "Epoch: 257 | Loss: 0.025764718651771545 \n",
            "Epoch: 258 | Loss: 0.025394471362233162 \n",
            "Epoch: 259 | Loss: 0.02502949722111225 \n",
            "Epoch: 260 | Loss: 0.024669712409377098 \n",
            "Epoch: 261 | Loss: 0.024315152317285538 \n",
            "Epoch: 262 | Loss: 0.023965822532773018 \n",
            "Epoch: 263 | Loss: 0.023621371015906334 \n",
            "Epoch: 264 | Loss: 0.023281855508685112 \n",
            "Epoch: 265 | Loss: 0.022947261109948158 \n",
            "Epoch: 266 | Loss: 0.022617438808083534 \n",
            "Epoch: 267 | Loss: 0.02229241281747818 \n",
            "Epoch: 268 | Loss: 0.02197207883000374 \n",
            "Epoch: 269 | Loss: 0.021656274795532227 \n",
            "Epoch: 270 | Loss: 0.021345039829611778 \n",
            "Epoch: 271 | Loss: 0.021038293838500977 \n",
            "Epoch: 272 | Loss: 0.020735936239361763 \n",
            "Epoch: 273 | Loss: 0.020437927916646004 \n",
            "Epoch: 274 | Loss: 0.020144246518611908 \n",
            "Epoch: 275 | Loss: 0.0198547150939703 \n",
            "Epoch: 276 | Loss: 0.01956935226917267 \n",
            "Epoch: 277 | Loss: 0.019288090988993645 \n",
            "Epoch: 278 | Loss: 0.019010845571756363 \n",
            "Epoch: 279 | Loss: 0.01873769983649254 \n",
            "Epoch: 280 | Loss: 0.018468420952558517 \n",
            "Epoch: 281 | Loss: 0.018202947452664375 \n",
            "Epoch: 282 | Loss: 0.017941322177648544 \n",
            "Epoch: 283 | Loss: 0.01768350787460804 \n",
            "Epoch: 284 | Loss: 0.017429322004318237 \n",
            "Epoch: 285 | Loss: 0.017178885638713837 \n",
            "Epoch: 286 | Loss: 0.01693199947476387 \n",
            "Epoch: 287 | Loss: 0.016688628122210503 \n",
            "Epoch: 288 | Loss: 0.01644880697131157 \n",
            "Epoch: 289 | Loss: 0.01621238887310028 \n",
            "Epoch: 290 | Loss: 0.015979431569576263 \n",
            "Epoch: 291 | Loss: 0.015749728307127953 \n",
            "Epoch: 292 | Loss: 0.015523433685302734 \n",
            "Epoch: 293 | Loss: 0.015300359576940536 \n",
            "Epoch: 294 | Loss: 0.015080420300364494 \n",
            "Epoch: 295 | Loss: 0.014863710850477219 \n",
            "Epoch: 296 | Loss: 0.014650062657892704 \n",
            "Epoch: 297 | Loss: 0.014439559541642666 \n",
            "Epoch: 298 | Loss: 0.014231981709599495 \n",
            "Epoch: 299 | Loss: 0.014027504250407219 \n",
            "Epoch: 300 | Loss: 0.013825839385390282 \n",
            "Epoch: 301 | Loss: 0.013627223670482635 \n",
            "Epoch: 302 | Loss: 0.01343137864023447 \n",
            "Epoch: 303 | Loss: 0.013238295912742615 \n",
            "Epoch: 304 | Loss: 0.013048077933490276 \n",
            "Epoch: 305 | Loss: 0.012860490940511227 \n",
            "Epoch: 306 | Loss: 0.012675701640546322 \n",
            "Epoch: 307 | Loss: 0.012493549846112728 \n",
            "Epoch: 308 | Loss: 0.012314018793404102 \n",
            "Epoch: 309 | Loss: 0.012136992067098618 \n",
            "Epoch: 310 | Loss: 0.011962557211518288 \n",
            "Epoch: 311 | Loss: 0.011790722608566284 \n",
            "Epoch: 312 | Loss: 0.011621265672147274 \n",
            "Epoch: 313 | Loss: 0.011454179883003235 \n",
            "Epoch: 314 | Loss: 0.01128963939845562 \n",
            "Epoch: 315 | Loss: 0.011127308011054993 \n",
            "Epoch: 316 | Loss: 0.010967430658638477 \n",
            "Epoch: 317 | Loss: 0.010809822008013725 \n",
            "Epoch: 318 | Loss: 0.010654505342245102 \n",
            "Epoch: 319 | Loss: 0.010501297190785408 \n",
            "Epoch: 320 | Loss: 0.010350458323955536 \n",
            "Epoch: 321 | Loss: 0.01020162645727396 \n",
            "Epoch: 322 | Loss: 0.010055096819996834 \n",
            "Epoch: 323 | Loss: 0.009910548105835915 \n",
            "Epoch: 324 | Loss: 0.00976813118904829 \n",
            "Epoch: 325 | Loss: 0.009627710096538067 \n",
            "Epoch: 326 | Loss: 0.009489397518336773 \n",
            "Epoch: 327 | Loss: 0.009353009052574635 \n",
            "Epoch: 328 | Loss: 0.009218567050993443 \n",
            "Epoch: 329 | Loss: 0.009086091071367264 \n",
            "Epoch: 330 | Loss: 0.008955525234341621 \n",
            "Epoch: 331 | Loss: 0.008826757781207561 \n",
            "Epoch: 332 | Loss: 0.008699924685060978 \n",
            "Epoch: 333 | Loss: 0.008574915118515491 \n",
            "Epoch: 334 | Loss: 0.008451670408248901 \n",
            "Epoch: 335 | Loss: 0.008330198004841805 \n",
            "Epoch: 336 | Loss: 0.008210480213165283 \n",
            "Epoch: 337 | Loss: 0.008092471398413181 \n",
            "Epoch: 338 | Loss: 0.007976224645972252 \n",
            "Epoch: 339 | Loss: 0.007861535996198654 \n",
            "Epoch: 340 | Loss: 0.007748573552817106 \n",
            "Epoch: 341 | Loss: 0.0076372213661670685 \n",
            "Epoch: 342 | Loss: 0.007527466863393784 \n",
            "Epoch: 343 | Loss: 0.007419292815029621 \n",
            "Epoch: 344 | Loss: 0.007312672678381205 \n",
            "Epoch: 345 | Loss: 0.007207569666206837 \n",
            "Epoch: 346 | Loss: 0.007103996817022562 \n",
            "Epoch: 347 | Loss: 0.007001893129199743 \n",
            "Epoch: 348 | Loss: 0.00690125348046422 \n",
            "Epoch: 349 | Loss: 0.006802062038332224 \n",
            "Epoch: 350 | Loss: 0.006704345345497131 \n",
            "Epoch: 351 | Loss: 0.006607959046959877 \n",
            "Epoch: 352 | Loss: 0.006512998137623072 \n",
            "Epoch: 353 | Loss: 0.0064194053411483765 \n",
            "Epoch: 354 | Loss: 0.0063271415419876575 \n",
            "Epoch: 355 | Loss: 0.006236182991415262 \n",
            "Epoch: 356 | Loss: 0.006146577186882496 \n",
            "Epoch: 357 | Loss: 0.006058225873857737 \n",
            "Epoch: 358 | Loss: 0.005971219390630722 \n",
            "Epoch: 359 | Loss: 0.0058853416703641415 \n",
            "Epoch: 360 | Loss: 0.005800790619105101 \n",
            "Epoch: 361 | Loss: 0.005717435386031866 \n",
            "Epoch: 362 | Loss: 0.005635229405015707 \n",
            "Epoch: 363 | Loss: 0.005554277449846268 \n",
            "Epoch: 364 | Loss: 0.0054744575172662735 \n",
            "Epoch: 365 | Loss: 0.005395749118179083 \n",
            "Epoch: 366 | Loss: 0.005318216979503632 \n",
            "Epoch: 367 | Loss: 0.005241750739514828 \n",
            "Epoch: 368 | Loss: 0.005166457034647465 \n",
            "Epoch: 369 | Loss: 0.00509221013635397 \n",
            "Epoch: 370 | Loss: 0.005019052419811487 \n",
            "Epoch: 371 | Loss: 0.00494688656181097 \n",
            "Epoch: 372 | Loss: 0.004875782877206802 \n",
            "Epoch: 373 | Loss: 0.004805746488273144 \n",
            "Epoch: 374 | Loss: 0.004736628383398056 \n",
            "Epoch: 375 | Loss: 0.004668571520596743 \n",
            "Epoch: 376 | Loss: 0.004601487889885902 \n",
            "Epoch: 377 | Loss: 0.004535352345556021 \n",
            "Epoch: 378 | Loss: 0.004470153711736202 \n",
            "Epoch: 379 | Loss: 0.004405898507684469 \n",
            "Epoch: 380 | Loss: 0.004342640750110149 \n",
            "Epoch: 381 | Loss: 0.004280193708837032 \n",
            "Epoch: 382 | Loss: 0.004218705929815769 \n",
            "Epoch: 383 | Loss: 0.004158048890531063 \n",
            "Epoch: 384 | Loss: 0.004098277539014816 \n",
            "Epoch: 385 | Loss: 0.0040394156239926815 \n",
            "Epoch: 386 | Loss: 0.00398135744035244 \n",
            "Epoch: 387 | Loss: 0.003924127668142319 \n",
            "Epoch: 388 | Loss: 0.003867713501676917 \n",
            "Epoch: 389 | Loss: 0.0038121454417705536 \n",
            "Epoch: 390 | Loss: 0.0037573466543108225 \n",
            "Epoch: 391 | Loss: 0.0037033730186522007 \n",
            "Epoch: 392 | Loss: 0.0036501481663435698 \n",
            "Epoch: 393 | Loss: 0.003597681410610676 \n",
            "Epoch: 394 | Loss: 0.0035459825303405523 \n",
            "Epoch: 395 | Loss: 0.0034950152039527893 \n",
            "Epoch: 396 | Loss: 0.0034447992220520973 \n",
            "Epoch: 397 | Loss: 0.0033952712547034025 \n",
            "Epoch: 398 | Loss: 0.00334648578427732 \n",
            "Epoch: 399 | Loss: 0.0032983776181936264 \n",
            "Epoch: 400 | Loss: 0.00325098168104887 \n",
            "Epoch: 401 | Loss: 0.0032042779494076967 \n",
            "Epoch: 402 | Loss: 0.0031582291703671217 \n",
            "Epoch: 403 | Loss: 0.0031128122936934233 \n",
            "Epoch: 404 | Loss: 0.0030681011267006397 \n",
            "Epoch: 405 | Loss: 0.003023994155228138 \n",
            "Epoch: 406 | Loss: 0.0029805246740579605 \n",
            "Epoch: 407 | Loss: 0.0029377005994319916 \n",
            "Epoch: 408 | Loss: 0.002895489800721407 \n",
            "Epoch: 409 | Loss: 0.0028538827318698168 \n",
            "Epoch: 410 | Loss: 0.002812854014337063 \n",
            "Epoch: 411 | Loss: 0.002772419946268201 \n",
            "Epoch: 412 | Loss: 0.002732573077082634 \n",
            "Epoch: 413 | Loss: 0.0026933043263852596 \n",
            "Epoch: 414 | Loss: 0.002654616255313158 \n",
            "Epoch: 415 | Loss: 0.00261644902639091 \n",
            "Epoch: 416 | Loss: 0.002578865271061659 \n",
            "Epoch: 417 | Loss: 0.0025418056175112724 \n",
            "Epoch: 418 | Loss: 0.0025052642449736595 \n",
            "Epoch: 419 | Loss: 0.0024692765437066555 \n",
            "Epoch: 420 | Loss: 0.0024337787181138992 \n",
            "Epoch: 421 | Loss: 0.0023987912572920322 \n",
            "Epoch: 422 | Loss: 0.0023643116001039743 \n",
            "Epoch: 423 | Loss: 0.0023303397465497255 \n",
            "Epoch: 424 | Loss: 0.002296836581081152 \n",
            "Epoch: 425 | Loss: 0.0022638251539319754 \n",
            "Epoch: 426 | Loss: 0.0022312996443361044 \n",
            "Epoch: 427 | Loss: 0.002199233276769519 \n",
            "Epoch: 428 | Loss: 0.002167648170143366 \n",
            "Epoch: 429 | Loss: 0.002136465860530734 \n",
            "Epoch: 430 | Loss: 0.0021057818084955215 \n",
            "Epoch: 431 | Loss: 0.0020755131263285875 \n",
            "Epoch: 432 | Loss: 0.0020456661004573107 \n",
            "Epoch: 433 | Loss: 0.002016272395849228 \n",
            "Epoch: 434 | Loss: 0.0019872887060046196 \n",
            "Epoch: 435 | Loss: 0.0019587313290685415 \n",
            "Epoch: 436 | Loss: 0.0019305819878354669 \n",
            "Epoch: 437 | Loss: 0.0019028268288820982 \n",
            "Epoch: 438 | Loss: 0.0018754990305751562 \n",
            "Epoch: 439 | Loss: 0.001848536659963429 \n",
            "Epoch: 440 | Loss: 0.001821977784857154 \n",
            "Epoch: 441 | Loss: 0.0017957935342565179 \n",
            "Epoch: 442 | Loss: 0.0017699945019558072 \n",
            "Epoch: 443 | Loss: 0.0017445544945076108 \n",
            "Epoch: 444 | Loss: 0.001719469903036952 \n",
            "Epoch: 445 | Loss: 0.0016947680851444602 \n",
            "Epoch: 446 | Loss: 0.0016704064328223467 \n",
            "Epoch: 447 | Loss: 0.0016464145155623555 \n",
            "Epoch: 448 | Loss: 0.0016227298183366656 \n",
            "Epoch: 449 | Loss: 0.0015994352288544178 \n",
            "Epoch: 450 | Loss: 0.0015764408744871616 \n",
            "Epoch: 451 | Loss: 0.0015537845902144909 \n",
            "Epoch: 452 | Loss: 0.0015314426273107529 \n",
            "Epoch: 453 | Loss: 0.0015094364061951637 \n",
            "Epoch: 454 | Loss: 0.0014877598732709885 \n",
            "Epoch: 455 | Loss: 0.0014663670444861054 \n",
            "Epoch: 456 | Loss: 0.0014452969189733267 \n",
            "Epoch: 457 | Loss: 0.0014245222555473447 \n",
            "Epoch: 458 | Loss: 0.0014040459645912051 \n",
            "Epoch: 459 | Loss: 0.0013838540762662888 \n",
            "Epoch: 460 | Loss: 0.0013639761600643396 \n",
            "Epoch: 461 | Loss: 0.0013443634379655123 \n",
            "Epoch: 462 | Loss: 0.0013250599149614573 \n",
            "Epoch: 463 | Loss: 0.001306022284552455 \n",
            "Epoch: 464 | Loss: 0.0012872472871094942 \n",
            "Epoch: 465 | Loss: 0.001268745050765574 \n",
            "Epoch: 466 | Loss: 0.00125050637871027 \n",
            "Epoch: 467 | Loss: 0.0012325587449595332 \n",
            "Epoch: 468 | Loss: 0.001214822055771947 \n",
            "Epoch: 469 | Loss: 0.0011973667424172163 \n",
            "Epoch: 470 | Loss: 0.0011801517102867365 \n",
            "Epoch: 471 | Loss: 0.0011631841771304607 \n",
            "Epoch: 472 | Loss: 0.0011464832350611687 \n",
            "Epoch: 473 | Loss: 0.0011300047626718879 \n",
            "Epoch: 474 | Loss: 0.001113763777539134 \n",
            "Epoch: 475 | Loss: 0.0010977580677717924 \n",
            "Epoch: 476 | Loss: 0.001081992406398058 \n",
            "Epoch: 477 | Loss: 0.0010664432775229216 \n",
            "Epoch: 478 | Loss: 0.00105111429002136 \n",
            "Epoch: 479 | Loss: 0.0010359971784055233 \n",
            "Epoch: 480 | Loss: 0.001021101139485836 \n",
            "Epoch: 481 | Loss: 0.0010064402595162392 \n",
            "Epoch: 482 | Loss: 0.0009919641306623816 \n",
            "Epoch: 483 | Loss: 0.0009777018567547202 \n",
            "Epoch: 484 | Loss: 0.0009636638569645584 \n",
            "Epoch: 485 | Loss: 0.0009498205035924911 \n",
            "Epoch: 486 | Loss: 0.000936168129555881 \n",
            "Epoch: 487 | Loss: 0.0009227098780684173 \n",
            "Epoch: 488 | Loss: 0.0009094403358176351 \n",
            "Epoch: 489 | Loss: 0.0008963746950030327 \n",
            "Epoch: 490 | Loss: 0.0008835038752295077 \n",
            "Epoch: 491 | Loss: 0.0008708033710718155 \n",
            "Epoch: 492 | Loss: 0.0008582953596487641 \n",
            "Epoch: 493 | Loss: 0.0008459555683657527 \n",
            "Epoch: 494 | Loss: 0.0008337891194969416 \n",
            "Epoch: 495 | Loss: 0.0008218124276027083 \n",
            "Epoch: 496 | Loss: 0.0008100001723505557 \n",
            "Epoch: 497 | Loss: 0.0007983687100932002 \n",
            "Epoch: 498 | Loss: 0.0007868784596212208 \n",
            "Epoch: 499 | Loss: 0.0007755880360491574 \n",
            "Prediction (after training) 4 7.967986106872559\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}